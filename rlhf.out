Collecting en-core-web-sm==3.7.1
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 100.3 MB/s eta 0:00:00
Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.4)
Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)
Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)
Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)
Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)
Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)
Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)
Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)
Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)
Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)
Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)
Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)
Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)
Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)
Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)
Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)
Requirement already satisfied: jinja2 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)
Requirement already satisfied: setuptools in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)
Requirement already satisfied: packaging>=20.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)
Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)
Requirement already satisfied: numpy>=1.19.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)
Requirement already satisfied: annotated-types>=0.4.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)
Requirement already satisfied: pydantic-core==2.16.3 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)
Requirement already satisfied: typing-extensions>=4.6.1 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)
Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)
Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)
Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)
Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)
Requirement already satisfied: MarkupSafe>=2.0 in /home/cs601-asandhu9/miniconda3/envs/ssm_hw7/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)
[38;5;2m✔ Download and installation successful[0m
You can now load the package via spacy.load('en_core_web_sm')
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `1`
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: angadsandhu (ouroboroos). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /home/cs601-asandhu9/hw7/wandb/run-20240402_191931-qi7kage1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rlhf
wandb: ⭐️ View project at https://wandb.ai/ouroboroos/hw7
wandb: 🚀 View run at https://wandb.ai/ouroboroos/hw7/runs/qi7kage1
INFO:__main__:Write to output directory: ./checkpoints/rlhf
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
INFO:__main__:Loading data ...
INFO:__main__:Loaded split train with 3853 total instances
INFO:__main__:Loaded split test with 948 total instances
INFO:__main__:Initializing models ...
  0%|          | 0/126 [00:00<?, ?it/s]INFO:__main__:[step 0] model checkpoint saved
INFO:__main__:Evaluating [step 0] ...

  0%|          | 0/474 [00:00<?, ?it/s][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 882 to 1024 to be a multiple of `config.attention_window`: 512

  0%|          | 1/474 [00:04<32:45,  4.16s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 924 to 1024 to be a multiple of `config.attention_window`: 512

  0%|          | 2/474 [00:05<19:13,  2.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 937 to 1024 to be a multiple of `config.attention_window`: 512

  1%|          | 3/474 [00:06<15:09,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512

  1%|          | 4/474 [00:08<14:08,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 912 to 1024 to be a multiple of `config.attention_window`: 512

  1%|          | 5/474 [00:09<13:14,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 976 to 1024 to be a multiple of `config.attention_window`: 512

  1%|▏         | 6/474 [00:11<12:49,  1.64s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512

  1%|▏         | 7/474 [00:12<11:39,  1.50s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 8/474 [00:14<11:38,  1.50s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 9/474 [00:15<11:46,  1.52s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 10/474 [00:17<11:53,  1.54s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 11/474 [00:18<11:13,  1.46s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512

  3%|▎         | 12/474 [00:19<10:20,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512

  3%|▎         | 13/474 [00:20<10:06,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 949 to 1024 to be a multiple of `config.attention_window`: 512

  3%|▎         | 14/474 [00:22<10:13,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 951 to 1024 to be a multiple of `config.attention_window`: 512

  3%|▎         | 15/474 [00:23<10:06,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 918 to 1024 to be a multiple of `config.attention_window`: 512

  3%|▎         | 16/474 [00:24<09:31,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 863 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▎         | 17/474 [00:25<09:10,  1.20s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 912 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▍         | 18/474 [00:27<09:55,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▍         | 19/474 [00:28<10:15,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 944 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▍         | 20/474 [00:30<10:56,  1.45s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 918 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▍         | 21/474 [00:31<10:51,  1.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▍         | 22/474 [00:33<11:16,  1.50s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▍         | 23/474 [00:34<10:19,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 925 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▌         | 24/474 [00:35<10:06,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▌         | 25/474 [00:36<09:46,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 929 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▌         | 26/474 [00:38<09:18,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512

  6%|▌         | 27/474 [00:38<08:27,  1.14s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512

  6%|▌         | 28/474 [00:40<09:04,  1.22s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512

  6%|▌         | 29/474 [00:41<09:41,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512

  6%|▋         | 30/474 [00:43<09:48,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 31/474 [00:44<09:01,  1.22s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 32/474 [00:45<09:28,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 33/474 [00:46<09:17,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 901 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 34/474 [00:47<08:46,  1.20s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 35/474 [00:49<09:08,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 903 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 36/474 [00:50<08:39,  1.19s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 37/474 [00:51<08:55,  1.22s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 964 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 38/474 [00:52<08:56,  1.23s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 39/474 [00:54<09:06,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 924 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 40/474 [00:55<08:46,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▊         | 41/474 [00:56<09:08,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 42/474 [00:58<09:19,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 599 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 43/474 [00:59<09:20,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 44/474 [01:00<09:18,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 854 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 45/474 [01:01<09:13,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512

 10%|▉         | 46/474 [01:03<09:32,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 565 to 1024 to be a multiple of `config.attention_window`: 512

 10%|▉         | 47/474 [01:04<09:08,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512

 10%|█         | 48/474 [01:06<09:29,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 512

 10%|█         | 49/474 [01:07<09:44,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█         | 50/474 [01:08<09:50,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█         | 51/474 [01:10<09:41,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 871 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█         | 52/474 [01:11<10:05,  1.43s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 874 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█         | 53/474 [01:13<10:15,  1.46s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█▏        | 54/474 [01:14<10:05,  1.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 55/474 [01:16<10:13,  1.46s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 56/474 [01:17<10:19,  1.48s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 57/474 [01:18<09:38,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 58/474 [01:20<09:08,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 996 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 59/474 [01:21<09:19,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 408 to 512 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 60/474 [01:22<08:38,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 61/474 [01:24<09:03,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 62/474 [01:25<08:43,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 63/474 [01:26<09:07,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▎        | 64/474 [01:28<09:13,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▎        | 65/474 [01:29<09:06,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 886 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▍        | 66/474 [01:30<08:24,  1.24s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▍        | 67/474 [01:31<08:32,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512

 14%|█▍        | 68/474 [01:32<08:05,  1.20s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512

 15%|█▍        | 69/474 [01:34<08:20,  1.23s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 965 to 1024 to be a multiple of `config.attention_window`: 512

 15%|█▍        | 70/474 [01:35<09:30,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512

 15%|█▍        | 71/474 [01:37<09:36,  1.43s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512

 15%|█▌        | 72/474 [01:38<09:27,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512

 15%|█▌        | 73/474 [01:40<10:09,  1.52s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 796 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 74/474 [01:41<09:30,  1.43s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 75/474 [01:43<09:35,  1.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 822 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 76/474 [01:44<09:19,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 989 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 77/474 [01:46<09:45,  1.47s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▋        | 78/474 [01:47<09:47,  1.48s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 864 to 1024 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 79/474 [01:48<09:20,  1.42s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 945 to 1024 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 80/474 [01:50<09:06,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 924 to 1024 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 81/474 [01:51<09:07,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 955 to 1024 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 82/474 [01:53<09:06,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 83/474 [01:54<09:04,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 84/474 [01:55<08:58,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 940 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 85/474 [01:57<08:58,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 86/474 [01:58<09:13,  1.43s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 87/474 [01:59<08:21,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▊        | 88/474 [02:01<08:31,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1029 to 1536 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 89/474 [02:03<10:14,  1.60s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 90/474 [02:04<09:50,  1.54s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 91/474 [02:06<10:15,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 872 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 92/474 [02:08<10:13,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 907 to 1024 to be a multiple of `config.attention_window`: 512

 20%|█▉        | 93/474 [02:09<09:50,  1.55s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 902 to 1024 to be a multiple of `config.attention_window`: 512

 20%|█▉        | 94/474 [02:10<09:28,  1.50s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512

 20%|██        | 95/474 [02:12<09:00,  1.43s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512

 20%|██        | 96/474 [02:13<08:53,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512

 20%|██        | 97/474 [02:14<08:32,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██        | 98/474 [02:16<08:37,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██        | 99/474 [02:17<08:40,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 886 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██        | 100/474 [02:18<08:41,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 893 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██▏       | 101/474 [02:20<08:10,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 920 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 102/474 [02:21<08:28,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 103/474 [02:23<08:38,  1.40s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 104/474 [02:24<08:42,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 105/474 [02:25<08:23,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 872 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 106/474 [02:27<08:30,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 107/474 [02:28<08:47,  1.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 974 to 1024 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 108/474 [02:30<08:32,  1.40s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 109/474 [02:31<08:31,  1.40s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 110/474 [02:33<08:56,  1.47s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 958 to 1024 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 111/474 [02:34<08:12,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▎       | 112/474 [02:35<08:18,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 910 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 113/474 [02:36<07:50,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 114/474 [02:37<07:30,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 906 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 115/474 [02:38<07:11,  1.20s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 116/474 [02:40<07:05,  1.19s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 909 to 1024 to be a multiple of `config.attention_window`: 512

 25%|██▍       | 117/474 [02:41<07:03,  1.19s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512

 25%|██▍       | 118/474 [02:42<07:09,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 964 to 1024 to be a multiple of `config.attention_window`: 512

 25%|██▌       | 119/474 [02:43<07:29,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 933 to 1024 to be a multiple of `config.attention_window`: 512

 25%|██▌       | 120/474 [02:45<07:38,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1008 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 121/474 [02:46<08:03,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 122/474 [02:48<08:06,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 123/474 [02:49<07:31,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 124/474 [02:50<07:21,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▋       | 125/474 [02:52<08:16,  1.42s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 126/474 [02:53<07:28,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 127/474 [02:54<07:20,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 732 to 1024 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 128/474 [02:55<07:23,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 129/474 [02:57<07:31,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 130/474 [02:58<07:30,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 131/474 [02:59<07:28,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 132/474 [03:01<07:54,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 661 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 133/474 [03:02<07:35,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 134/474 [03:03<07:26,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 933 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 135/474 [03:05<07:28,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512

 29%|██▊       | 136/474 [03:06<07:21,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 911 to 1024 to be a multiple of `config.attention_window`: 512

 29%|██▉       | 137/474 [03:07<07:22,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512

 29%|██▉       | 138/474 [03:09<07:28,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512

 29%|██▉       | 139/474 [03:10<07:06,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512

 30%|██▉       | 140/474 [03:11<06:49,  1.23s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512

 30%|██▉       | 141/474 [03:12<06:50,  1.23s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512

 30%|██▉       | 142/474 [03:14<07:13,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512

 30%|███       | 143/474 [03:15<07:06,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512

 30%|███       | 144/474 [03:16<07:15,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512

 31%|███       | 145/474 [03:17<06:47,  1.24s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███       | 146/474 [03:19<06:57,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 924 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███       | 147/474 [03:20<06:55,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███       | 148/474 [03:21<06:58,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 844 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███▏      | 149/474 [03:23<07:01,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 150/474 [03:24<07:19,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 151/474 [03:26<07:26,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 152/474 [03:27<06:51,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 153/474 [03:28<07:07,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 154/474 [03:29<07:07,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 155/474 [03:31<07:30,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 817 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 156/474 [03:32<06:58,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 157/474 [03:33<06:40,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 158/474 [03:34<06:35,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512

 34%|███▎      | 159/474 [03:36<06:14,  1.19s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 974 to 1024 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 160/474 [03:37<06:27,  1.23s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 161/474 [03:38<06:38,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 162/474 [03:40<06:58,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 163/474 [03:41<06:54,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 843 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▍      | 164/474 [03:42<06:50,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▍      | 165/474 [03:44<06:38,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 959 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▌      | 166/474 [03:45<06:42,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▌      | 167/474 [03:46<06:37,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 929 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▌      | 168/474 [03:48<06:39,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 965 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▌      | 169/474 [03:49<06:45,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▌      | 170/474 [03:50<06:35,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▌      | 171/474 [03:52<07:28,  1.48s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 800 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▋      | 172/474 [03:53<07:22,  1.47s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 945 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▋      | 173/474 [03:55<07:15,  1.45s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 872 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 174/474 [03:56<07:01,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 930 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 175/474 [03:57<06:52,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 176/474 [03:59<06:49,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 996 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 177/474 [04:00<07:01,  1.42s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 178/474 [04:02<07:02,  1.43s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 179/474 [04:03<07:04,  1.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 180/474 [04:04<06:31,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 966 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 181/474 [04:06<06:39,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 182/474 [04:07<06:31,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 512

 39%|███▊      | 183/474 [04:09<06:40,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 976 to 1024 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 184/474 [04:10<06:45,  1.40s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 185/474 [04:11<06:19,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 186/474 [04:12<05:49,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 908 to 1024 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 187/474 [04:13<05:57,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512

 40%|███▉      | 188/474 [04:15<05:52,  1.23s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 920 to 1024 to be a multiple of `config.attention_window`: 512

 40%|███▉      | 189/474 [04:16<05:55,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 906 to 1024 to be a multiple of `config.attention_window`: 512

 40%|████      | 190/474 [04:17<05:52,  1.24s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 883 to 1024 to be a multiple of `config.attention_window`: 512

 40%|████      | 191/474 [04:18<05:44,  1.22s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512

 41%|████      | 192/474 [04:19<05:40,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 939 to 1024 to be a multiple of `config.attention_window`: 512

 41%|████      | 193/474 [04:21<05:30,  1.18s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512

 41%|████      | 194/474 [04:22<05:38,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512

 41%|████      | 195/474 [04:23<05:36,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 968 to 1024 to be a multiple of `config.attention_window`: 512

 41%|████▏     | 196/474 [04:25<06:00,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 810 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 197/474 [04:26<05:52,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 904 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 198/474 [04:27<06:00,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 199/474 [04:29<06:06,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 868 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 200/474 [04:30<06:09,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 201/474 [04:31<06:05,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 933 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 202/474 [04:33<05:58,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 883 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 203/474 [04:34<05:40,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 966 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 204/474 [04:35<05:46,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 903 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 205/474 [04:36<05:42,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 934 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 206/474 [04:38<05:40,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 732 to 1024 to be a multiple of `config.attention_window`: 512

 44%|████▎     | 207/474 [04:39<05:48,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 851 to 1024 to be a multiple of `config.attention_window`: 512

 44%|████▍     | 208/474 [04:40<05:47,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512

 44%|████▍     | 209/474 [04:42<05:45,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 806 to 1024 to be a multiple of `config.attention_window`: 512

 44%|████▍     | 210/474 [04:43<05:41,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▍     | 211/474 [04:44<05:52,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▍     | 212/474 [04:45<05:43,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▍     | 213/474 [04:47<05:47,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 806 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▌     | 214/474 [04:49<06:13,  1.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▌     | 215/474 [04:50<05:42,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 216/474 [04:51<05:51,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 217/474 [04:52<05:37,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 218/474 [04:53<05:29,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 918 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 219/474 [04:55<05:32,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▋     | 220/474 [04:56<05:44,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 935 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 221/474 [04:58<05:39,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 535 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 222/474 [04:59<05:34,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 964 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 223/474 [05:00<05:42,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 224/474 [05:02<05:40,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 905 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 225/474 [05:03<05:48,  1.40s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 825 to 1024 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 226/474 [05:04<05:37,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 936 to 1024 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 227/474 [05:06<05:41,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 228/474 [05:07<05:28,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 945 to 1024 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 229/474 [05:08<05:27,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512

 49%|████▊     | 230/474 [05:10<05:27,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 948 to 1024 to be a multiple of `config.attention_window`: 512

 49%|████▊     | 231/474 [05:11<05:22,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512

 49%|████▉     | 232/474 [05:13<05:31,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 512

 49%|████▉     | 233/474 [05:14<05:11,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1045 to 1536 to be a multiple of `config.attention_window`: 512

 49%|████▉     | 234/474 [05:16<06:07,  1.53s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512

 50%|████▉     | 235/474 [05:17<05:54,  1.48s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 901 to 1024 to be a multiple of `config.attention_window`: 512

 50%|████▉     | 236/474 [05:18<05:39,  1.43s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 925 to 1024 to be a multiple of `config.attention_window`: 512

 50%|█████     | 237/474 [05:20<05:24,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512

 50%|█████     | 238/474 [05:21<05:28,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512

 50%|█████     | 239/474 [05:23<05:54,  1.51s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████     | 240/474 [05:25<06:08,  1.57s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 629 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████     | 241/474 [05:26<05:46,  1.49s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████     | 242/474 [05:27<05:28,  1.42s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████▏    | 243/474 [05:28<05:12,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████▏    | 244/474 [05:29<04:54,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 245/474 [05:31<04:53,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 951 to 1024 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 246/474 [05:32<04:54,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 247/474 [05:33<04:58,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 248/474 [05:35<05:12,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 249/474 [05:36<05:03,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 949 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 250/474 [05:38<05:16,  1.42s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 983 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 251/474 [05:39<05:16,  1.42s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 252/474 [05:40<05:02,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 253/474 [05:41<04:30,  1.23s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 898 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▎    | 254/474 [05:43<04:41,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 852 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 255/474 [05:44<04:35,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 256/474 [05:45<04:31,  1.24s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 257/474 [05:47<04:34,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 875 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 258/474 [05:48<04:43,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 912 to 1024 to be a multiple of `config.attention_window`: 512

 55%|█████▍    | 259/474 [05:49<04:41,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512

 55%|█████▍    | 260/474 [05:51<04:45,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 778 to 1024 to be a multiple of `config.attention_window`: 512

 55%|█████▌    | 261/474 [05:52<04:42,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512

 55%|█████▌    | 262/474 [05:53<04:33,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512

 55%|█████▌    | 263/474 [05:54<04:09,  1.18s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512

 56%|█████▌    | 264/474 [05:55<03:55,  1.12s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 912 to 1024 to be a multiple of `config.attention_window`: 512

 56%|█████▌    | 265/474 [05:56<03:58,  1.14s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512

 56%|█████▌    | 266/474 [05:58<04:26,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512

 56%|█████▋    | 267/474 [05:59<04:43,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 918 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 268/474 [06:01<04:29,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 269/474 [06:02<04:18,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 932 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 270/474 [06:03<04:31,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 924 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 271/474 [06:05<04:30,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 272/474 [06:06<04:27,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 948 to 1024 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 273/474 [06:07<04:38,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 274/474 [06:09<04:25,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 275/474 [06:10<04:24,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 985 to 1024 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 276/474 [06:11<04:31,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 955 to 1024 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 277/474 [06:13<04:38,  1.42s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▊    | 278/474 [06:14<04:43,  1.45s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 279/474 [06:16<04:40,  1.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 280/474 [06:17<04:40,  1.45s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 967 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 281/474 [06:19<04:25,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 607 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 282/474 [06:20<04:13,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 374 to 512 to be a multiple of `config.attention_window`: 512

 60%|█████▉    | 283/474 [06:21<03:58,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 987 to 1024 to be a multiple of `config.attention_window`: 512

 60%|█████▉    | 284/474 [06:22<04:01,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 813 to 1024 to be a multiple of `config.attention_window`: 512

 60%|██████    | 285/474 [06:24<04:07,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512

 60%|██████    | 286/474 [06:25<04:07,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 951 to 1024 to be a multiple of `config.attention_window`: 512

 61%|██████    | 287/474 [06:26<04:01,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512

 61%|██████    | 288/474 [06:27<03:33,  1.15s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 925 to 1024 to be a multiple of `config.attention_window`: 512

 61%|██████    | 289/474 [06:28<03:54,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512

 61%|██████    | 290/474 [06:30<04:06,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 920 to 1024 to be a multiple of `config.attention_window`: 512

 61%|██████▏   | 291/474 [06:31<03:58,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 914 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 292/474 [06:33<04:00,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 293/474 [06:34<04:00,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 926 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 294/474 [06:35<04:03,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 295/474 [06:37<04:00,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 817 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 296/474 [06:38<03:57,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 297/474 [06:39<03:43,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 298/474 [06:41<03:55,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 299/474 [06:42<03:35,  1.23s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 300/474 [06:43<03:41,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512

 64%|██████▎   | 301/474 [06:44<03:21,  1.16s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 907 to 1024 to be a multiple of `config.attention_window`: 512

 64%|██████▎   | 302/474 [06:45<03:19,  1.16s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512

 64%|██████▍   | 303/474 [06:46<03:21,  1.18s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512

 64%|██████▍   | 304/474 [06:48<03:41,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512

 64%|██████▍   | 305/474 [06:49<03:22,  1.20s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▍   | 306/474 [06:50<03:31,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▍   | 307/474 [06:51<03:30,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 893 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▍   | 308/474 [06:53<03:42,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 845 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▌   | 309/474 [06:54<03:45,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 967 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▌   | 310/474 [06:56<03:45,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 311/474 [06:57<03:47,  1.40s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 920 to 1024 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 312/474 [06:58<03:37,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 313/474 [06:59<03:14,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 314/474 [07:01<03:28,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512

 66%|██████▋   | 315/474 [07:02<03:20,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 926 to 1024 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 316/474 [07:03<03:29,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 863 to 1024 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 317/474 [07:05<03:34,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 318/474 [07:06<03:01,  1.16s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 709 to 1024 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 319/474 [07:07<03:08,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 900 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 320/474 [07:08<03:10,  1.24s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 321/474 [07:10<03:19,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 322/474 [07:11<03:11,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 323/474 [07:12<03:17,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 893 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 324/474 [07:14<03:26,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 723 to 1024 to be a multiple of `config.attention_window`: 512

 69%|██████▊   | 325/474 [07:16<03:38,  1.47s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1009 to 1024 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 326/474 [07:17<03:39,  1.49s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 327/474 [07:18<03:16,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 901 to 1024 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 328/474 [07:19<03:14,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 329/474 [07:20<02:54,  1.20s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512

 70%|██████▉   | 330/474 [07:21<02:47,  1.17s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 758 to 1024 to be a multiple of `config.attention_window`: 512

 70%|██████▉   | 331/474 [07:23<03:14,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 632 to 1024 to be a multiple of `config.attention_window`: 512

 70%|███████   | 332/474 [07:24<03:04,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512

 70%|███████   | 333/474 [07:26<03:00,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512

 70%|███████   | 334/474 [07:27<02:59,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512

 71%|███████   | 335/474 [07:28<02:59,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 971 to 1024 to be a multiple of `config.attention_window`: 512

 71%|███████   | 336/474 [07:29<02:54,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 907 to 1024 to be a multiple of `config.attention_window`: 512

 71%|███████   | 337/474 [07:31<02:54,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512

 71%|███████▏  | 338/474 [07:32<02:58,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 608 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 339/474 [07:33<02:50,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 340/474 [07:34<02:50,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 341/474 [07:35<02:34,  1.16s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 342/474 [07:37<02:43,  1.24s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 343/474 [07:38<02:49,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 962 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 344/474 [07:40<02:49,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 958 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 345/474 [07:41<02:52,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 890 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 346/474 [07:42<02:55,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 347/474 [07:44<02:46,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 929 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 348/474 [07:45<02:48,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▎  | 349/474 [07:46<02:38,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 350/474 [07:47<02:37,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 850 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 351/474 [07:49<02:47,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 352/474 [07:50<02:26,  1.20s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 868 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 353/474 [07:51<02:20,  1.16s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 951 to 1024 to be a multiple of `config.attention_window`: 512

 75%|███████▍  | 354/474 [07:52<02:25,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512

 75%|███████▍  | 355/474 [07:53<02:20,  1.18s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512

 75%|███████▌  | 356/474 [07:55<02:23,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512

 75%|███████▌  | 357/474 [07:56<02:27,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 358/474 [07:57<02:33,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 359/474 [07:59<02:29,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 360/474 [08:00<02:13,  1.17s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 361/474 [08:01<02:14,  1.19s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 965 to 1024 to be a multiple of `config.attention_window`: 512

 76%|███████▋  | 362/474 [08:02<02:24,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 363/474 [08:03<02:17,  1.24s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 364/474 [08:05<02:24,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 866 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 365/474 [08:06<02:10,  1.20s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 971 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 366/474 [08:07<02:17,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 367/474 [08:08<02:09,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 368/474 [08:10<02:09,  1.23s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 369/474 [08:11<02:05,  1.20s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 370/474 [08:12<02:13,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 572 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 371/474 [08:14<02:14,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1008 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 372/474 [08:15<02:22,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512

 79%|███████▊  | 373/474 [08:17<02:28,  1.47s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512

 79%|███████▉  | 374/474 [08:18<02:10,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 855 to 1024 to be a multiple of `config.attention_window`: 512

 79%|███████▉  | 375/474 [08:19<02:13,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512

 79%|███████▉  | 376/474 [08:21<02:12,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512

 80%|███████▉  | 377/474 [08:22<02:16,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512

 80%|███████▉  | 378/474 [08:23<02:09,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512

 80%|███████▉  | 379/474 [08:24<01:57,  1.23s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 926 to 1024 to be a multiple of `config.attention_window`: 512

 80%|████████  | 380/474 [08:26<02:10,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512

 80%|████████  | 381/474 [08:27<01:59,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512

 81%|████████  | 382/474 [08:28<01:59,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512

 81%|████████  | 383/474 [08:29<01:42,  1.13s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512

 81%|████████  | 384/474 [08:30<01:42,  1.14s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 931 to 1024 to be a multiple of `config.attention_window`: 512

 81%|████████  | 385/474 [08:32<01:47,  1.21s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1000 to 1024 to be a multiple of `config.attention_window`: 512

 81%|████████▏ | 386/474 [08:33<01:43,  1.17s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 387/474 [08:34<01:42,  1.18s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 743 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 388/474 [08:36<01:59,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 389/474 [08:37<01:51,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 971 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 390/474 [08:38<01:51,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 391/474 [08:40<01:48,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 941 to 1024 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 392/474 [08:41<01:48,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 349 to 512 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 393/474 [08:42<01:32,  1.14s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 394/474 [08:43<01:32,  1.15s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 569 to 1024 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 395/474 [08:44<01:26,  1.10s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▎ | 396/474 [08:45<01:29,  1.14s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 397/474 [08:46<01:35,  1.24s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 398/474 [08:47<01:27,  1.15s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 911 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 399/474 [08:49<01:35,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 841 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 400/474 [08:50<01:36,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512

 85%|████████▍ | 401/474 [08:51<01:25,  1.17s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512

 85%|████████▍ | 402/474 [08:53<01:30,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 936 to 1024 to be a multiple of `config.attention_window`: 512

 85%|████████▌ | 403/474 [08:54<01:31,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512

 85%|████████▌ | 404/474 [08:55<01:30,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 941 to 1024 to be a multiple of `config.attention_window`: 512

 85%|████████▌ | 405/474 [08:57<01:26,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 962 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▌ | 406/474 [08:58<01:27,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▌ | 407/474 [08:59<01:26,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 968 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▌ | 408/474 [09:00<01:24,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▋ | 409/474 [09:02<01:21,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▋ | 410/474 [09:03<01:20,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 930 to 1024 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 411/474 [09:04<01:20,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 412/474 [09:06<01:29,  1.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 413/474 [09:07<01:25,  1.40s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 414/474 [09:08<01:17,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 415/474 [09:10<01:16,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 416/474 [09:11<01:15,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 939 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 417/474 [09:12<01:16,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 418/474 [09:14<01:18,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 955 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 419/474 [09:16<01:19,  1.45s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▊ | 420/474 [09:17<01:17,  1.43s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 941 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 421/474 [09:18<01:12,  1.37s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 422/474 [09:19<01:09,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 877 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 423/474 [09:21<01:07,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 424/474 [09:22<01:09,  1.40s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 903 to 1024 to be a multiple of `config.attention_window`: 512

 90%|████████▉ | 425/474 [09:24<01:06,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 873 to 1024 to be a multiple of `config.attention_window`: 512

 90%|████████▉ | 426/474 [09:25<01:06,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512

 90%|█████████ | 427/474 [09:27<01:07,  1.43s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 901 to 1024 to be a multiple of `config.attention_window`: 512

 90%|█████████ | 428/474 [09:28<01:02,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 429/474 [09:29<01:03,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 430/474 [09:31<01:03,  1.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 431/474 [09:32<00:59,  1.39s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 866 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 432/474 [09:33<00:53,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 965 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████▏| 433/474 [09:34<00:52,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 916 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 434/474 [09:36<00:59,  1.48s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 435/474 [09:38<00:55,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 908 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 436/474 [09:39<00:51,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 437/474 [09:40<00:51,  1.40s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 661 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 438/474 [09:42<00:50,  1.41s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 439/474 [09:43<00:48,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 929 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 440/474 [09:45<00:48,  1.42s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 441/474 [09:46<00:47,  1.44s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 442/474 [09:48<00:47,  1.47s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 666 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 443/474 [09:49<00:45,  1.48s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 512

 94%|█████████▎| 444/474 [09:50<00:43,  1.46s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 865 to 1024 to be a multiple of `config.attention_window`: 512

 94%|█████████▍| 445/474 [09:52<00:41,  1.45s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 859 to 1024 to be a multiple of `config.attention_window`: 512

 94%|█████████▍| 446/474 [09:53<00:37,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 872 to 1024 to be a multiple of `config.attention_window`: 512

 94%|█████████▍| 447/474 [09:54<00:35,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 853 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▍| 448/474 [09:56<00:34,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▍| 449/474 [09:57<00:33,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▍| 450/474 [09:58<00:32,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▌| 451/474 [10:00<00:30,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▌| 452/474 [10:01<00:28,  1.29s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 998 to 1024 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 453/474 [10:02<00:29,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 935 to 1024 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 454/474 [10:04<00:26,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 988 to 1024 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 455/474 [10:05<00:25,  1.36s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 456/474 [10:06<00:23,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 513 to 1024 to be a multiple of `config.attention_window`: 512

 96%|█████████▋| 457/474 [10:08<00:22,  1.34s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 445 to 512 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 458/474 [10:09<00:21,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 459/474 [10:10<00:19,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 551 to 1024 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 460/474 [10:12<00:18,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1001 to 1024 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 461/474 [10:13<00:17,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 462/474 [10:14<00:15,  1.33s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 463/474 [10:15<00:13,  1.25s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 958 to 1024 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 464/474 [10:17<00:12,  1.28s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 465/474 [10:18<00:10,  1.18s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 912 to 1024 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 466/474 [10:19<00:10,  1.26s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512

 99%|█████████▊| 467/474 [10:20<00:08,  1.27s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 932 to 1024 to be a multiple of `config.attention_window`: 512

 99%|█████████▊| 468/474 [10:22<00:08,  1.35s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512

 99%|█████████▉| 469/474 [10:23<00:06,  1.31s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512

 99%|█████████▉| 470/474 [10:24<00:05,  1.32s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512

 99%|█████████▉| 471/474 [10:26<00:04,  1.38s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512

100%|█████████▉| 472/474 [10:27<00:02,  1.30s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512

100%|█████████▉| 473/474 [10:29<00:01,  1.45s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512

100%|██████████| 474/474 [10:30<00:00,  1.43s/it][A100%|██████████| 474/474 [10:30<00:00,  1.33s/it]
INFO:__main__:Evaluated [step 0] rewards = -2.2989
INFO:__main__:Best ckpt updated to [step 0]
INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512
  1%|          | 1/126 [10:54<22:43:28, 654.47s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 976 to 1024 to be a multiple of `config.attention_window`: 512
  2%|▏         | 2/126 [11:10<9:36:14, 278.83s/it] INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512
  2%|▏         | 3/126 [11:26<5:25:25, 158.74s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512
  3%|▎         | 4/126 [11:42<3:28:28, 102.53s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 945 to 1024 to be a multiple of `config.attention_window`: 512
  4%|▍         | 5/126 [11:58<2:23:45, 71.29s/it] INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 907 to 1024 to be a multiple of `config.attention_window`: 512
  5%|▍         | 6/126 [12:14<1:44:53, 52.44s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 914 to 1024 to be a multiple of `config.attention_window`: 512
  6%|▌         | 7/126 [12:30<1:20:15, 40.46s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 512
  6%|▋         | 8/126 [12:46<1:04:31, 32.81s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512
  7%|▋         | 9/126 [13:02<53:45, 27.57s/it]  INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512
  8%|▊         | 10/126 [13:18<46:22, 23.99s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1051 to 1536 to be a multiple of `config.attention_window`: 512
  9%|▊         | 11/126 [13:35<41:47, 21.80s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512
 10%|▉         | 12/126 [13:51<37:54, 19.95s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1021 to 1024 to be a multiple of `config.attention_window`: 512
 10%|█         | 13/126 [14:07<35:37, 18.91s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 822 to 1024 to be a multiple of `config.attention_window`: 512
 11%|█         | 14/126 [14:23<33:36, 18.00s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512
 12%|█▏        | 15/126 [14:39<32:17, 17.46s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512
 13%|█▎        | 16/126 [14:55<31:14, 17.04s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512
 13%|█▎        | 17/126 [15:11<30:18, 16.68s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 971 to 1024 to be a multiple of `config.attention_window`: 512
 14%|█▍        | 18/126 [15:27<29:39, 16.48s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512
 15%|█▌        | 19/126 [15:43<29:05, 16.31s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1043 to 1536 to be a multiple of `config.attention_window`: 512
 16%|█▌        | 20/126 [16:00<29:20, 16.61s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 937 to 1024 to be a multiple of `config.attention_window`: 512
 17%|█▋        | 21/126 [16:16<28:41, 16.39s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 972 to 1024 to be a multiple of `config.attention_window`: 512
 17%|█▋        | 22/126 [16:32<28:07, 16.23s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 694 to 1024 to be a multiple of `config.attention_window`: 512
 18%|█▊        | 23/126 [16:48<27:45, 16.17s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 765 to 1024 to be a multiple of `config.attention_window`: 512
 19%|█▉        | 24/126 [17:05<27:37, 16.25s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 985 to 1024 to be a multiple of `config.attention_window`: 512
 20%|█▉        | 25/126 [17:21<27:15, 16.19s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1053 to 1536 to be a multiple of `config.attention_window`: 512
 21%|██        | 26/126 [17:38<27:39, 16.59s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 512
 21%|██▏       | 27/126 [17:55<27:19, 16.56s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 972 to 1024 to be a multiple of `config.attention_window`: 512
 22%|██▏       | 28/126 [18:11<26:43, 16.37s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 939 to 1024 to be a multiple of `config.attention_window`: 512
 23%|██▎       | 29/126 [18:26<26:14, 16.24s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512
 24%|██▍       | 30/126 [18:43<25:56, 16.21s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512
 25%|██▍       | 31/126 [18:59<25:49, 16.31s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 944 to 1024 to be a multiple of `config.attention_window`: 512
 25%|██▌       | 32/126 [19:16<25:37, 16.36s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512
 26%|██▌       | 33/126 [19:32<25:16, 16.31s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512
 27%|██▋       | 34/126 [19:48<25:06, 16.37s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 986 to 1024 to be a multiple of `config.attention_window`: 512
 28%|██▊       | 35/126 [20:04<24:40, 16.27s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 962 to 1024 to be a multiple of `config.attention_window`: 512
 29%|██▊       | 36/126 [20:20<24:16, 16.18s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512
 29%|██▉       | 37/126 [20:37<24:09, 16.29s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1072 to 1536 to be a multiple of `config.attention_window`: 512
 30%|███       | 38/126 [20:54<24:25, 16.65s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512
 31%|███       | 39/126 [21:11<23:58, 16.53s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1027 to 1536 to be a multiple of `config.attention_window`: 512
 32%|███▏      | 40/126 [21:28<24:06, 16.82s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 765 to 1024 to be a multiple of `config.attention_window`: 512
 33%|███▎      | 41/126 [21:45<23:42, 16.74s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512
 33%|███▎      | 42/126 [22:01<23:13, 16.59s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 987 to 1024 to be a multiple of `config.attention_window`: 512
 34%|███▍      | 43/126 [22:17<22:50, 16.51s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1003 to 1024 to be a multiple of `config.attention_window`: 512
 35%|███▍      | 44/126 [22:34<22:33, 16.51s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1048 to 1536 to be a multiple of `config.attention_window`: 512
 36%|███▌      | 45/126 [22:51<22:41, 16.81s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512
 37%|███▋      | 46/126 [23:08<22:17, 16.72s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 512
 37%|███▋      | 47/126 [23:24<21:56, 16.66s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 873 to 1024 to be a multiple of `config.attention_window`: 512
 38%|███▊      | 48/126 [23:41<21:29, 16.54s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 983 to 1024 to be a multiple of `config.attention_window`: 512
 39%|███▉      | 49/126 [23:57<21:12, 16.53s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512
 40%|███▉      | 50/126 [24:14<20:56, 16.53s/it]INFO:__main__:[step 50] model checkpoint saved
INFO:__main__:Evaluating [step 50] ...

  0%|          | 0/474 [00:00<?, ?it/s][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 893 to 1024 to be a multiple of `config.attention_window`: 512

  0%|          | 1/474 [00:01<11:04,  1.40s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 990 to 1024 to be a multiple of `config.attention_window`: 512

  0%|          | 2/474 [00:03<13:19,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1003 to 1024 to be a multiple of `config.attention_window`: 512

  1%|          | 3/474 [00:05<13:37,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512

  1%|          | 4/474 [00:06<13:40,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 968 to 1024 to be a multiple of `config.attention_window`: 512

  1%|          | 5/474 [00:08<14:05,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512

  1%|▏         | 6/474 [00:10<13:13,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512

  1%|▏         | 7/474 [00:11<12:44,  1.64s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 8/474 [00:13<13:21,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 9/474 [00:15<13:45,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 992 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 10/474 [00:17<14:01,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 11/474 [00:18<13:21,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 779 to 1024 to be a multiple of `config.attention_window`: 512

  3%|▎         | 12/474 [00:20<13:24,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512

  3%|▎         | 13/474 [00:22<13:43,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1033 to 1536 to be a multiple of `config.attention_window`: 512

  3%|▎         | 14/474 [00:24<14:37,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512

  3%|▎         | 15/474 [00:27<15:14,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1006 to 1024 to be a multiple of `config.attention_window`: 512

  3%|▎         | 16/474 [00:28<14:59,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 909 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▎         | 17/474 [00:30<14:19,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▍         | 18/474 [00:32<14:01,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1070 to 1536 to be a multiple of `config.attention_window`: 512

  4%|▍         | 19/474 [00:34<14:47,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▍         | 20/474 [00:36<14:32,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 959 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▍         | 21/474 [00:37<13:19,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1065 to 1536 to be a multiple of `config.attention_window`: 512

  5%|▍         | 22/474 [00:40<14:15,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1001 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▍         | 23/474 [00:41<14:13,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▌         | 24/474 [00:43<14:12,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▌         | 25/474 [00:45<13:16,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▌         | 26/474 [00:46<12:16,  1.64s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512

  6%|▌         | 27/474 [00:47<11:12,  1.50s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 580 to 1024 to be a multiple of `config.attention_window`: 512

  6%|▌         | 28/474 [00:49<12:04,  1.62s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...

  6%|▌         | 29/474 [00:51<12:37,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512

  6%|▋         | 30/474 [00:53<12:32,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 31/474 [00:54<11:17,  1.53s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1006 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 32/474 [00:56<12:05,  1.64s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 33/474 [00:57<11:40,  1.59s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 34/474 [00:59<11:08,  1.52s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 962 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 35/474 [01:00<11:19,  1.55s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 36/474 [01:02<10:56,  1.50s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 37/474 [01:03<10:56,  1.50s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 988 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 38/474 [01:05<10:41,  1.47s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 39/474 [01:06<10:31,  1.45s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1042 to 1536 to be a multiple of `config.attention_window`: 512

  8%|▊         | 40/474 [01:08<12:07,  1.68s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 766 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▊         | 41/474 [01:10<12:34,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 42/474 [01:11<11:40,  1.62s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 43/474 [01:13<12:15,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 44/474 [01:15<12:15,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 45/474 [01:17<12:39,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512

 10%|▉         | 46/474 [01:19<12:13,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512

 10%|▉         | 47/474 [01:20<11:29,  1.62s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1021 to 1024 to be a multiple of `config.attention_window`: 512

 10%|█         | 48/474 [01:22<12:05,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512

 10%|█         | 49/474 [01:24<12:29,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512

 11%|█         | 50/474 [01:26<13:23,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█         | 51/474 [01:28<12:49,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 901 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█         | 52/474 [01:29<12:57,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 937 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█         | 53/474 [01:31<13:02,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1027 to 1536 to be a multiple of `config.attention_window`: 512

 11%|█▏        | 54/474 [01:34<13:42,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 55/474 [01:35<13:33,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 774 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 56/474 [01:37<13:25,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 711 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 57/474 [01:39<12:20,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 58/474 [01:40<11:57,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 996 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 59/474 [01:42<11:16,  1.63s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 60/474 [01:43<10:28,  1.52s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 947 to 1024 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 61/474 [01:45<10:28,  1.52s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 62/474 [01:46<10:46,  1.57s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1021 to 1024 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 63/474 [01:48<11:27,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 752 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▎        | 64/474 [01:50<11:40,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▎        | 65/474 [01:52<12:02,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▍        | 66/474 [01:53<11:45,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▍        | 67/474 [01:55<11:25,  1.68s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512

 14%|█▍        | 68/474 [01:57<11:14,  1.66s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 897 to 1024 to be a multiple of `config.attention_window`: 512

 15%|█▍        | 69/474 [01:58<10:46,  1.60s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 512

 15%|█▍        | 70/474 [02:00<11:23,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1029 to 1536 to be a multiple of `config.attention_window`: 512

 15%|█▍        | 71/474 [02:02<12:23,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512

 15%|█▌        | 72/474 [02:04<11:54,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1044 to 1536 to be a multiple of `config.attention_window`: 512

 15%|█▌        | 73/474 [02:06<12:31,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 74/474 [02:07<11:38,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 741 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 75/474 [02:09<11:54,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 910 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 76/474 [02:11<11:59,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1037 to 1536 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 77/474 [02:13<12:43,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1087 to 1536 to be a multiple of `config.attention_window`: 512

 16%|█▋        | 78/474 [02:16<13:13,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 79/474 [02:17<12:59,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1040 to 1536 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 80/474 [02:20<13:11,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 924 to 1024 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 81/474 [02:21<11:57,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 82/474 [02:23<12:38,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 878 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 83/474 [02:25<12:31,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 84/474 [02:27<12:26,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 945 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 85/474 [02:28<11:24,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 86/474 [02:30<11:01,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 87/474 [02:31<09:36,  1.49s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▊        | 88/474 [02:32<09:43,  1.51s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 89/474 [02:35<11:00,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 90/474 [02:36<10:43,  1.68s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 91/474 [02:38<11:07,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 962 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 92/474 [02:40<11:23,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 974 to 1024 to be a multiple of `config.attention_window`: 512

 20%|█▉        | 93/474 [02:42<11:33,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512

 20%|█▉        | 94/474 [02:43<10:53,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1044 to 1536 to be a multiple of `config.attention_window`: 512

 20%|██        | 95/474 [02:46<11:45,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512

 20%|██        | 96/474 [02:47<11:12,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 781 to 1024 to be a multiple of `config.attention_window`: 512

 20%|██        | 97/474 [02:49<11:23,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██        | 98/474 [02:51<10:48,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 726 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██        | 99/474 [02:52<11:06,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██        | 100/474 [02:54<11:18,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1003 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██▏       | 101/474 [02:56<11:15,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 984 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 102/474 [02:58<11:23,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 103/474 [03:00<12:01,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 104/474 [03:02<11:38,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 105/474 [03:04<11:39,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 930 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 106/474 [03:06<11:10,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 999 to 1024 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 107/474 [03:07<10:47,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1043 to 1536 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 108/474 [03:09<11:16,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1048 to 1536 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 109/474 [03:11<11:52,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 110/474 [03:13<11:00,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 989 to 1024 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 111/474 [03:14<10:06,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▎       | 112/474 [03:16<10:29,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 937 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 113/474 [03:18<10:45,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 986 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 114/474 [03:20<10:24,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 920 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 115/474 [03:21<09:32,  1.60s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 116/474 [03:23<10:10,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 512

 25%|██▍       | 117/474 [03:24<09:50,  1.65s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1045 to 1536 to be a multiple of `config.attention_window`: 512

 25%|██▍       | 118/474 [03:27<10:47,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...

 25%|██▌       | 119/474 [03:29<10:55,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512

 25%|██▌       | 120/474 [03:30<11:00,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1064 to 1536 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 121/474 [03:33<11:35,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 951 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 122/474 [03:35<11:23,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 123/474 [03:36<11:18,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 124/474 [03:38<10:25,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 974 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▋       | 125/474 [03:40<10:36,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 126/474 [03:41<09:35,  1.65s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 866 to 1024 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 127/474 [03:43<10:00,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 843 to 1024 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 128/474 [03:45<09:54,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 129/474 [03:47<10:11,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 130/474 [03:49<10:53,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 131/474 [03:51<10:52,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1000 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 132/474 [03:53<10:50,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 133/474 [03:54<10:01,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 935 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 134/474 [03:56<09:34,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1029 to 1536 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 135/474 [03:58<10:25,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1052 to 1536 to be a multiple of `config.attention_window`: 512

 29%|██▊       | 136/474 [04:00<11:00,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1003 to 1024 to be a multiple of `config.attention_window`: 512

 29%|██▉       | 137/474 [04:02<10:52,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 967 to 1024 to be a multiple of `config.attention_window`: 512

 29%|██▉       | 138/474 [04:04<10:40,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512

 29%|██▉       | 139/474 [04:05<09:18,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 713 to 1024 to be a multiple of `config.attention_window`: 512

 30%|██▉       | 140/474 [04:06<09:08,  1.64s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512

 30%|██▉       | 141/474 [04:08<08:45,  1.58s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512

 30%|██▉       | 142/474 [04:10<09:15,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512

 30%|███       | 143/474 [04:11<09:04,  1.64s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512

 30%|███       | 144/474 [04:13<08:50,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512

 31%|███       | 145/474 [04:14<08:49,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███       | 146/474 [04:16<08:45,  1.60s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███       | 147/474 [04:18<09:13,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███       | 148/474 [04:20<09:32,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 890 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███▏      | 149/474 [04:21<09:17,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 150/474 [04:23<09:33,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 151/474 [04:26<10:13,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 152/474 [04:27<09:12,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 153/474 [04:28<09:03,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 912 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 154/474 [04:30<08:35,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 155/474 [04:32<09:02,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 865 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 156/474 [04:34<09:21,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 157/474 [04:35<09:05,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 158/474 [04:37<09:22,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512

 34%|███▎      | 159/474 [04:39<09:04,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1056 to 1536 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 160/474 [04:41<09:45,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 765 to 1024 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 161/474 [04:43<09:47,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 162/474 [04:45<09:24,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 163/474 [04:47<09:31,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 866 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▍      | 164/474 [04:48<08:49,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▍      | 165/474 [04:49<08:17,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 959 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▌      | 166/474 [04:51<08:18,  1.62s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▌      | 167/474 [04:52<07:59,  1.56s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512

 35%|███▌      | 168/474 [04:55<08:57,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1021 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▌      | 169/474 [04:56<08:52,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 955 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▌      | 170/474 [04:58<08:31,  1.68s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1013 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▌      | 171/474 [05:00<08:46,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 841 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▋      | 172/474 [05:02<09:00,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 989 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▋      | 173/474 [05:03<08:35,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 877 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 174/474 [05:05<08:20,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1009 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 175/474 [05:07<08:40,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 176/474 [05:09<08:52,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 987 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 177/474 [05:10<08:22,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 178/474 [05:12<08:39,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 179/474 [05:14<08:26,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 180/474 [05:15<07:39,  1.56s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 976 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 181/474 [05:16<07:31,  1.54s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 182/474 [05:18<08:02,  1.65s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512

 39%|███▊      | 183/474 [05:20<08:49,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1050 to 1536 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 184/474 [05:23<09:21,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 185/474 [05:24<08:36,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 186/474 [05:25<07:34,  1.58s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1042 to 1536 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 187/474 [05:27<08:26,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512

 40%|███▉      | 188/474 [05:29<08:32,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 937 to 1024 to be a multiple of `config.attention_window`: 512

 40%|███▉      | 189/474 [05:31<08:26,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512

 40%|████      | 190/474 [05:32<08:07,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 962 to 1024 to be a multiple of `config.attention_window`: 512

 40%|████      | 191/474 [05:34<07:51,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 41%|████      | 192/474 [05:36<08:35,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512

 41%|████      | 193/474 [05:38<08:16,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512

 41%|████      | 194/474 [05:39<07:52,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1066 to 1536 to be a multiple of `config.attention_window`: 512

 41%|████      | 195/474 [05:42<08:34,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512

 41%|████▏     | 196/474 [05:43<08:20,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 906 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 197/474 [05:45<08:26,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 198/474 [05:47<08:30,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 779 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 199/474 [05:49<08:32,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 945 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 200/474 [05:51<08:33,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 201/474 [05:53<08:33,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 202/474 [05:54<08:12,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 203/474 [05:56<08:11,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1052 to 1536 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 204/474 [05:58<08:41,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 205/474 [06:00<08:36,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 949 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 206/474 [06:02<07:54,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 790 to 1024 to be a multiple of `config.attention_window`: 512

 44%|████▎     | 207/474 [06:04<08:04,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512

 44%|████▍     | 208/474 [06:06<08:08,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1040 to 1536 to be a multiple of `config.attention_window`: 512

 44%|████▍     | 209/474 [06:08<08:21,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512

 44%|████▍     | 210/474 [06:09<08:19,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1062 to 1536 to be a multiple of `config.attention_window`: 512

 45%|████▍     | 211/474 [06:12<08:42,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 788 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▍     | 212/474 [06:14<08:33,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 743 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▍     | 213/474 [06:15<08:26,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▌     | 214/474 [06:17<08:21,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▌     | 215/474 [06:19<07:27,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 989 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 216/474 [06:20<07:13,  1.68s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 217/474 [06:22<06:50,  1.60s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 218/474 [06:23<06:46,  1.59s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1006 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 219/474 [06:25<07:09,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▋     | 220/474 [06:27<07:25,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 221/474 [06:29<07:35,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 589 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 222/474 [06:30<07:12,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1009 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 223/474 [06:32<07:14,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1066 to 1536 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 224/474 [06:34<07:48,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 962 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 225/474 [06:36<07:49,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 860 to 1024 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 226/474 [06:38<07:18,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 227/474 [06:40<07:27,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 228/474 [06:41<07:00,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 229/474 [06:43<07:34,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512

 49%|████▊     | 230/474 [06:45<07:36,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512

 49%|████▊     | 231/474 [06:47<07:58,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512

 49%|████▉     | 232/474 [06:49<07:51,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512

 49%|████▉     | 233/474 [06:51<07:11,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1064 to 1536 to be a multiple of `config.attention_window`: 512

 49%|████▉     | 234/474 [06:53<07:38,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512

 50%|████▉     | 235/474 [06:55<07:17,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1021 to 1024 to be a multiple of `config.attention_window`: 512

 50%|████▉     | 236/474 [06:56<07:17,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 925 to 1024 to be a multiple of `config.attention_window`: 512

 50%|█████     | 237/474 [06:58<06:40,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 909 to 1024 to be a multiple of `config.attention_window`: 512

 50%|█████     | 238/474 [06:59<06:24,  1.63s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 512

 50%|█████     | 239/474 [07:01<06:42,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████     | 240/474 [07:03<06:54,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████     | 241/474 [07:05<06:39,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512

 51%|█████     | 242/474 [07:07<07:12,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████▏    | 243/474 [07:08<06:26,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████▏    | 244/474 [07:10<06:25,  1.68s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 987 to 1024 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 245/474 [07:12<06:32,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 951 to 1024 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 246/474 [07:13<06:02,  1.59s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 247/474 [07:15<06:22,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 987 to 1024 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 248/474 [07:17<06:35,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 249/474 [07:19<06:43,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 250/474 [07:21<06:49,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1060 to 1536 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 251/474 [07:23<07:11,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 252/474 [07:24<06:47,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 253/474 [07:26<06:49,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 989 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▎    | 254/474 [07:28<06:50,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 872 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 255/474 [07:30<06:23,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 843 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 256/474 [07:31<06:12,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1048 to 1536 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 257/474 [07:33<06:42,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 875 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 258/474 [07:35<06:28,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 55%|█████▍    | 259/474 [07:37<06:52,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512

 55%|█████▍    | 260/474 [07:39<07:08,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 869 to 1024 to be a multiple of `config.attention_window`: 512

 55%|█████▌    | 261/474 [07:41<06:39,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512

 55%|█████▌    | 262/474 [07:43<06:58,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512

 55%|█████▌    | 263/474 [07:45<06:52,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512

 56%|█████▌    | 264/474 [07:46<05:57,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512

 56%|█████▌    | 265/474 [07:48<05:52,  1.68s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 512

 56%|█████▌    | 266/474 [07:50<06:04,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1068 to 1536 to be a multiple of `config.attention_window`: 512

 56%|█████▋    | 267/474 [07:52<06:31,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 983 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 268/474 [07:54<06:11,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 694 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 269/474 [07:55<05:48,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 270/474 [07:57<05:35,  1.65s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1010 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 271/474 [07:59<05:49,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 272/474 [08:00<05:36,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1042 to 1536 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 273/474 [08:02<06:07,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 274/474 [08:04<06:01,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 734 to 1024 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 275/474 [08:06<06:05,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1053 to 1536 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 276/474 [08:08<06:25,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1010 to 1024 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 277/474 [08:10<06:21,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▊    | 278/474 [08:12<06:06,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1037 to 1536 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 279/474 [08:14<06:24,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 280/474 [08:16<06:18,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1037 to 1536 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 281/474 [08:18<06:29,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 282/474 [08:20<06:00,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512

 60%|█████▉    | 283/474 [08:21<05:27,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1084 to 1536 to be a multiple of `config.attention_window`: 512

 60%|█████▉    | 284/474 [08:23<05:53,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 863 to 1024 to be a multiple of `config.attention_window`: 512

 60%|██████    | 285/474 [08:25<05:41,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1045 to 1536 to be a multiple of `config.attention_window`: 512

 60%|██████    | 286/474 [08:27<06:02,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512

 61%|██████    | 287/474 [08:29<06:14,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512

 61%|██████    | 288/474 [08:30<05:24,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1001 to 1024 to be a multiple of `config.attention_window`: 512

 61%|██████    | 289/474 [08:32<05:32,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 786 to 1024 to be a multiple of `config.attention_window`: 512

 61%|██████    | 290/474 [08:34<05:36,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1022 to 1024 to be a multiple of `config.attention_window`: 512

 61%|██████▏   | 291/474 [08:36<05:39,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 292/474 [08:38<05:34,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 293/474 [08:40<05:36,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 294/474 [08:41<05:10,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 295/474 [08:43<05:18,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 296/474 [08:45<05:23,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 297/474 [08:47<05:09,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 778 to 1024 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 298/474 [08:48<05:15,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 299/474 [08:50<04:49,  1.66s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 300/474 [08:51<04:45,  1.64s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512

 64%|██████▎   | 301/474 [08:53<04:39,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1002 to 1024 to be a multiple of `config.attention_window`: 512

 64%|██████▎   | 302/474 [08:55<04:44,  1.65s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512

 64%|██████▍   | 303/474 [08:57<04:55,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1050 to 1536 to be a multiple of `config.attention_window`: 512

 64%|██████▍   | 304/474 [08:59<05:17,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512

 64%|██████▍   | 305/474 [09:00<04:33,  1.62s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▍   | 306/474 [09:01<04:21,  1.56s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▍   | 307/474 [09:03<04:29,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 988 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▍   | 308/474 [09:05<04:41,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 929 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▌   | 309/474 [09:06<04:34,  1.66s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 967 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▌   | 310/474 [09:08<04:44,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 311/474 [09:10<04:48,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 312/474 [09:12<04:42,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 313/474 [09:13<04:09,  1.55s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1067 to 1536 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 314/474 [09:15<04:38,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512

 66%|██████▋   | 315/474 [09:17<04:25,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 316/474 [09:19<04:48,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 317/474 [09:21<04:50,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 318/474 [09:22<04:08,  1.59s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 319/474 [09:24<04:21,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 930 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 320/474 [09:25<04:09,  1.62s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 321/474 [09:27<04:20,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 322/474 [09:28<03:52,  1.53s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 323/474 [09:30<04:08,  1.64s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 324/474 [09:32<04:18,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512

 69%|██████▊   | 325/474 [09:34<04:24,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1086 to 1536 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 326/474 [09:36<04:41,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 327/474 [09:37<04:15,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 940 to 1024 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 328/474 [09:39<04:21,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 538 to 1024 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 329/474 [09:41<04:07,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512

 70%|██████▉   | 330/474 [09:42<03:54,  1.63s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 512

 70%|██████▉   | 331/474 [09:44<04:04,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512

 70%|███████   | 332/474 [09:46<04:11,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512

 70%|███████   | 333/474 [09:48<04:13,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512

 70%|███████   | 334/474 [09:50<04:06,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 979 to 1024 to be a multiple of `config.attention_window`: 512

 71%|███████   | 335/474 [09:51<04:00,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1084 to 1536 to be a multiple of `config.attention_window`: 512

 71%|███████   | 336/474 [09:54<04:18,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512

 71%|███████   | 337/474 [09:55<04:00,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512

 71%|███████▏  | 338/474 [09:57<04:04,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 339/474 [09:59<03:55,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 340/474 [10:00<03:59,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 341/474 [10:02<03:33,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 726 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 342/474 [10:04<03:43,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 734 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 343/474 [10:05<03:50,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1035 to 1536 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 344/474 [10:07<04:00,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1039 to 1536 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 345/474 [10:10<04:12,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 346/474 [10:12<04:08,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 347/474 [10:13<03:42,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 348/474 [10:15<03:46,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▎  | 349/474 [10:16<03:28,  1.66s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 350/474 [10:18<03:25,  1.66s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 873 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 351/474 [10:20<03:29,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 352/474 [10:21<03:07,  1.54s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 353/474 [10:22<03:10,  1.57s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 985 to 1024 to be a multiple of `config.attention_window`: 512

 75%|███████▍  | 354/474 [10:24<03:20,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512

 75%|███████▍  | 355/474 [10:26<03:19,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512

 75%|███████▌  | 356/474 [10:28<03:25,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512

 75%|███████▌  | 357/474 [10:30<03:22,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 358/474 [10:32<03:27,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 359/474 [10:33<03:11,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 360/474 [10:34<03:06,  1.63s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 361/474 [10:36<02:50,  1.51s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1003 to 1024 to be a multiple of `config.attention_window`: 512

 76%|███████▋  | 362/474 [10:37<02:57,  1.58s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 363/474 [10:39<02:47,  1.51s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 364/474 [10:40<02:42,  1.48s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 365/474 [10:42<02:39,  1.47s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 366/474 [10:44<03:02,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 367/474 [10:45<02:55,  1.64s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 715 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 368/474 [10:47<02:52,  1.63s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 369/474 [10:48<02:46,  1.59s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 692 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 370/474 [10:50<02:54,  1.68s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 371/474 [10:52<02:55,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1052 to 1536 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 372/474 [10:54<03:07,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512

 79%|███████▊  | 373/474 [10:56<03:08,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512

 79%|███████▉  | 374/474 [10:57<02:49,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 932 to 1024 to be a multiple of `config.attention_window`: 512

 79%|███████▉  | 375/474 [10:59<02:54,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 988 to 1024 to be a multiple of `config.attention_window`: 512

 79%|███████▉  | 376/474 [11:01<02:56,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 968 to 1024 to be a multiple of `config.attention_window`: 512

 80%|███████▉  | 377/474 [11:03<02:58,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 512

 80%|███████▉  | 378/474 [11:04<02:40,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512

 80%|███████▉  | 379/474 [11:06<02:24,  1.52s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512

 80%|████████  | 380/474 [11:08<02:33,  1.63s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512

 80%|████████  | 381/474 [11:09<02:28,  1.59s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512

 81%|████████  | 382/474 [11:11<02:25,  1.58s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512

 81%|████████  | 383/474 [11:12<02:11,  1.45s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1032 to 1536 to be a multiple of `config.attention_window`: 512

 81%|████████  | 384/474 [11:14<02:20,  1.56s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512

 81%|████████  | 385/474 [11:15<02:27,  1.66s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1169 to 1536 to be a multiple of `config.attention_window`: 512

 81%|████████▏ | 386/474 [11:18<02:40,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 387/474 [11:19<02:26,  1.69s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 388/474 [11:20<02:17,  1.60s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 389/474 [11:22<02:16,  1.60s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1071 to 1536 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 390/474 [11:24<02:29,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 391/474 [11:26<02:31,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 960 to 1024 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 392/474 [11:28<02:20,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 393/474 [11:29<02:05,  1.54s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 394/474 [11:31<02:10,  1.63s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 395/474 [11:32<02:06,  1.60s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▎ | 396/474 [11:33<01:59,  1.53s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 397/474 [11:35<01:55,  1.50s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 398/474 [11:37<02:02,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 399/474 [11:38<02:02,  1.63s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 859 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 400/474 [11:40<02:06,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512

 85%|████████▍ | 401/474 [11:42<01:56,  1.59s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 911 to 1024 to be a multiple of `config.attention_window`: 512

 85%|████████▍ | 402/474 [11:43<01:52,  1.57s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512

 85%|████████▌ | 403/474 [11:45<02:04,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512

 85%|████████▌ | 404/474 [11:47<01:59,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1027 to 1536 to be a multiple of `config.attention_window`: 512

 85%|████████▌ | 405/474 [11:49<02:05,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▌ | 406/474 [11:51<02:05,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▌ | 407/474 [11:53<02:04,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1070 to 1536 to be a multiple of `config.attention_window`: 512

 86%|████████▌ | 408/474 [11:55<02:09,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▋ | 409/474 [11:57<02:06,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▋ | 410/474 [11:58<01:53,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1033 to 1536 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 411/474 [12:00<01:57,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 412/474 [12:03<02:01,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 728 to 1024 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 413/474 [12:04<01:58,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 414/474 [12:06<01:55,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 415/474 [12:08<01:46,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 709 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 416/474 [12:10<01:45,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 979 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 417/474 [12:11<01:37,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1054 to 1536 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 418/474 [12:13<01:44,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 419/474 [12:15<01:42,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 992 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▊ | 420/474 [12:17<01:39,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1035 to 1536 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 421/474 [12:19<01:43,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 422/474 [12:21<01:40,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 964 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 423/474 [12:23<01:38,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1010 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 424/474 [12:25<01:35,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 936 to 1024 to be a multiple of `config.attention_window`: 512

 90%|████████▉ | 425/474 [12:26<01:26,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512

 90%|████████▉ | 426/474 [12:28<01:26,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512

 90%|█████████ | 427/474 [12:30<01:26,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 941 to 1024 to be a multiple of `config.attention_window`: 512

 90%|█████████ | 428/474 [12:32<01:18,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 429/474 [12:34<01:23,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 999 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 430/474 [12:36<01:22,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 431/474 [12:37<01:16,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 900 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 432/474 [12:39<01:10,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512

 91%|█████████▏| 433/474 [12:41<01:15,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 978 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 434/474 [12:43<01:14,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 976 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 435/474 [12:45<01:10,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 436/474 [12:46<01:09,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 959 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 437/474 [12:48<01:05,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 438/474 [12:50<01:05,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 986 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 439/474 [12:51<00:59,  1.70s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 989 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 440/474 [12:53<00:59,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 441/474 [12:55<01:00,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 903 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 442/474 [12:57<00:59,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 443/474 [12:59<00:54,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1042 to 1536 to be a multiple of `config.attention_window`: 512

 94%|█████████▎| 444/474 [13:01<00:53,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 962 to 1024 to be a multiple of `config.attention_window`: 512

 94%|█████████▍| 445/474 [13:02<00:51,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512

 94%|█████████▍| 446/474 [13:04<00:50,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 971 to 1024 to be a multiple of `config.attention_window`: 512

 94%|█████████▍| 447/474 [13:06<00:49,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 896 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▍| 448/474 [13:08<00:44,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512

 95%|█████████▍| 449/474 [13:10<00:46,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1068 to 1536 to be a multiple of `config.attention_window`: 512

 95%|█████████▍| 450/474 [13:12<00:47,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▌| 451/474 [13:14<00:44,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▌| 452/474 [13:16<00:42,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1065 to 1536 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 453/474 [13:18<00:42,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 984 to 1024 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 454/474 [13:20<00:38,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1082 to 1536 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 455/474 [13:22<00:38,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 456/474 [13:24<00:33,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512

 96%|█████████▋| 457/474 [13:25<00:30,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 458/474 [13:26<00:26,  1.63s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 955 to 1024 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 459/474 [13:28<00:25,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 460/474 [13:30<00:24,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1139 to 1536 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 461/474 [13:32<00:24,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1005 to 1024 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 462/474 [13:34<00:22,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 463/474 [13:36<00:18,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1044 to 1536 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 464/474 [13:38<00:18,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 465/474 [13:39<00:14,  1.65s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 466/474 [13:41<00:13,  1.72s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 958 to 1024 to be a multiple of `config.attention_window`: 512

 99%|█████████▊| 467/474 [13:42<00:11,  1.62s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512

 99%|█████████▊| 468/474 [13:44<00:10,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512

 99%|█████████▉| 469/474 [13:45<00:07,  1.57s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512

 99%|█████████▉| 470/474 [13:47<00:06,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512

 99%|█████████▉| 471/474 [13:49<00:05,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 619 to 1024 to be a multiple of `config.attention_window`: 512

100%|█████████▉| 472/474 [13:51<00:03,  1.61s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512

100%|█████████▉| 473/474 [13:52<00:01,  1.66s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512

100%|██████████| 474/474 [13:54<00:00,  1.63s/it][A100%|██████████| 474/474 [13:54<00:00,  1.76s/it]
INFO:__main__:Evaluated [step 50] rewards = -2.2871
INFO:__main__:Best ckpt updated to [step 50]
INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1048 to 1536 to be a multiple of `config.attention_window`: 512
 40%|████      | 51/126 [38:30<5:35:45, 268.61s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1044 to 1536 to be a multiple of `config.attention_window`: 512
 41%|████▏     | 52/126 [38:48<3:58:22, 193.28s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 770 to 1024 to be a multiple of `config.attention_window`: 512
 42%|████▏     | 53/126 [39:04<2:50:38, 140.25s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512
 43%|████▎     | 54/126 [39:22<2:04:06, 103.43s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1051 to 1536 to be a multiple of `config.attention_window`: 512
 44%|████▎     | 55/126 [39:39<1:31:53, 77.66s/it] INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512
 44%|████▍     | 56/126 [39:57<1:09:33, 59.62s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 471 to 512 to be a multiple of `config.attention_window`: 512
 45%|████▌     | 57/126 [40:13<53:20, 46.38s/it]  INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1002 to 1024 to be a multiple of `config.attention_window`: 512
 46%|████▌     | 58/126 [40:29<42:24, 37.42s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1095 to 1536 to be a multiple of `config.attention_window`: 512
 47%|████▋     | 59/126 [40:47<35:07, 31.45s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512
 48%|████▊     | 60/126 [41:02<29:20, 26.68s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512
 48%|████▊     | 61/126 [41:18<25:16, 23.34s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1056 to 1536 to be a multiple of `config.attention_window`: 512
 49%|████▉     | 62/126 [41:35<23:02, 21.60s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512
 50%|█████     | 63/126 [41:52<21:04, 20.08s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 794 to 1024 to be a multiple of `config.attention_window`: 512
 51%|█████     | 64/126 [42:08<19:38, 19.01s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512
 52%|█████▏    | 65/126 [42:25<18:34, 18.27s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512
 52%|█████▏    | 66/126 [42:41<17:45, 17.75s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512
 53%|█████▎    | 67/126 [42:59<17:23, 17.68s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512
 54%|█████▍    | 68/126 [43:14<16:28, 17.04s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1037 to 1536 to be a multiple of `config.attention_window`: 512
 55%|█████▍    | 69/126 [43:32<16:19, 17.19s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1042 to 1536 to be a multiple of `config.attention_window`: 512
 56%|█████▌    | 70/126 [43:49<16:08, 17.29s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512
 56%|█████▋    | 71/126 [44:07<15:55, 17.37s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 863 to 1024 to be a multiple of `config.attention_window`: 512
 57%|█████▋    | 72/126 [44:24<15:24, 17.11s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1065 to 1536 to be a multiple of `config.attention_window`: 512
 58%|█████▊    | 73/126 [44:41<15:13, 17.24s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512
 59%|█████▊    | 74/126 [44:56<14:26, 16.67s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512
 60%|█████▉    | 75/126 [45:13<14:07, 16.63s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1043 to 1536 to be a multiple of `config.attention_window`: 512
 60%|██████    | 76/126 [45:30<14:04, 16.89s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1038 to 1536 to be a multiple of `config.attention_window`: 512
 61%|██████    | 77/126 [45:48<13:56, 17.08s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512
 62%|██████▏   | 78/126 [46:04<13:31, 16.91s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512
 63%|██████▎   | 79/126 [46:21<13:09, 16.80s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 889 to 1024 to be a multiple of `config.attention_window`: 512
 63%|██████▎   | 80/126 [46:38<12:48, 16.71s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1069 to 1536 to be a multiple of `config.attention_window`: 512
 64%|██████▍   | 81/126 [46:55<12:43, 16.96s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 808 to 1024 to be a multiple of `config.attention_window`: 512
 65%|██████▌   | 82/126 [47:12<12:20, 16.83s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1070 to 1536 to be a multiple of `config.attention_window`: 512
 66%|██████▌   | 83/126 [47:29<12:12, 17.04s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1055 to 1536 to be a multiple of `config.attention_window`: 512
 67%|██████▋   | 84/126 [47:47<12:01, 17.19s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512
 67%|██████▋   | 85/126 [48:03<11:36, 16.99s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512
 68%|██████▊   | 86/126 [48:21<11:25, 17.14s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 772 to 1024 to be a multiple of `config.attention_window`: 512
 69%|██████▉   | 87/126 [48:37<11:01, 16.96s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1051 to 1536 to be a multiple of `config.attention_window`: 512
 70%|██████▉   | 88/126 [48:55<10:50, 17.12s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1106 to 1536 to be a multiple of `config.attention_window`: 512
 71%|███████   | 89/126 [49:12<10:37, 17.24s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512
 71%|███████▏  | 90/126 [49:29<10:12, 17.03s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512
 72%|███████▏  | 91/126 [49:45<09:50, 16.88s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 910 to 1024 to be a multiple of `config.attention_window`: 512
 73%|███████▎  | 92/126 [50:02<09:30, 16.77s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1065 to 1536 to be a multiple of `config.attention_window`: 512
 74%|███████▍  | 93/126 [50:19<09:20, 16.99s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1051 to 1536 to be a multiple of `config.attention_window`: 512
 75%|███████▍  | 94/126 [50:37<09:08, 17.15s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512
 75%|███████▌  | 95/126 [50:53<08:45, 16.96s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 990 to 1024 to be a multiple of `config.attention_window`: 512
 76%|███████▌  | 96/126 [51:10<08:24, 16.83s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1065 to 1536 to be a multiple of `config.attention_window`: 512
 77%|███████▋  | 97/126 [51:27<08:14, 17.03s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1055 to 1536 to be a multiple of `config.attention_window`: 512
 78%|███████▊  | 98/126 [51:45<08:01, 17.18s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1007 to 1024 to be a multiple of `config.attention_window`: 512
 79%|███████▊  | 99/126 [52:01<07:38, 16.98s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 765 to 1024 to be a multiple of `config.attention_window`: 512
 79%|███████▉  | 100/126 [52:18<07:17, 16.84s/it]INFO:__main__:[step 100] model checkpoint saved
INFO:__main__:Evaluating [step 100] ...

  0%|          | 0/474 [00:00<?, ?it/s][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 906 to 1024 to be a multiple of `config.attention_window`: 512

  0%|          | 1/474 [00:01<15:04,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1026 to 1536 to be a multiple of `config.attention_window`: 512

  0%|          | 2/474 [00:04<16:25,  2.09s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 996 to 1024 to be a multiple of `config.attention_window`: 512

  1%|          | 3/474 [00:05<15:09,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 734 to 1024 to be a multiple of `config.attention_window`: 512

  1%|          | 4/474 [00:07<15:02,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512

  1%|          | 5/474 [00:09<14:58,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

  1%|▏         | 6/474 [00:11<15:41,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 766 to 1024 to be a multiple of `config.attention_window`: 512

  1%|▏         | 7/474 [00:13<15:24,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 8/474 [00:15<15:12,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 9/474 [00:17<15:02,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 990 to 1024 to be a multiple of `config.attention_window`: 512

  2%|▏         | 10/474 [00:19<14:54,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1048 to 1536 to be a multiple of `config.attention_window`: 512

  2%|▏         | 11/474 [00:21<15:30,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 788 to 1024 to be a multiple of `config.attention_window`: 512

  3%|▎         | 12/474 [00:23<15:03,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1032 to 1536 to be a multiple of `config.attention_window`: 512

  3%|▎         | 13/474 [00:25<15:35,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512

  3%|▎         | 14/474 [00:27<15:55,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512

  3%|▎         | 15/474 [00:30<16:09,  2.11s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1006 to 1024 to be a multiple of `config.attention_window`: 512

  3%|▎         | 16/474 [00:32<15:38,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 989 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▎         | 17/474 [00:33<15:15,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512

  4%|▍         | 18/474 [00:36<15:39,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1069 to 1536 to be a multiple of `config.attention_window`: 512

  4%|▍         | 19/474 [00:38<15:56,  2.10s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1026 to 1536 to be a multiple of `config.attention_window`: 512

  4%|▍         | 20/474 [00:40<16:07,  2.13s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 997 to 1024 to be a multiple of `config.attention_window`: 512

  4%|▍         | 21/474 [00:42<15:33,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1062 to 1536 to be a multiple of `config.attention_window`: 512

  5%|▍         | 22/474 [00:44<15:49,  2.10s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1000 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▍         | 23/474 [00:46<15:20,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 987 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▌         | 24/474 [00:48<15:00,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512

  5%|▌         | 25/474 [00:50<14:46,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1047 to 1536 to be a multiple of `config.attention_window`: 512

  5%|▌         | 26/474 [00:52<15:15,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512

  6%|▌         | 27/474 [00:53<13:18,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512

  6%|▌         | 28/474 [00:55<13:33,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1027 to 1536 to be a multiple of `config.attention_window`: 512

  6%|▌         | 29/474 [00:57<14:23,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 788 to 1024 to be a multiple of `config.attention_window`: 512

  6%|▋         | 30/474 [00:59<14:17,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 31/474 [01:01<14:12,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1053 to 1536 to be a multiple of `config.attention_window`: 512

  7%|▋         | 32/474 [01:03<14:47,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 734 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 33/474 [01:05<14:31,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 34/474 [01:07<13:20,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1005 to 1024 to be a multiple of `config.attention_window`: 512

  7%|▋         | 35/474 [01:09<13:29,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1029 to 1536 to be a multiple of `config.attention_window`: 512

  8%|▊         | 36/474 [01:11<14:13,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 37/474 [01:13<14:05,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1062 to 1536 to be a multiple of `config.attention_window`: 512

  8%|▊         | 38/474 [01:15<14:36,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512

  8%|▊         | 39/474 [01:17<14:21,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1051 to 1536 to be a multiple of `config.attention_window`: 512

  8%|▊         | 40/474 [01:19<14:48,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 770 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▊         | 41/474 [01:21<14:28,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 42/474 [01:23<14:14,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 707 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 43/474 [01:25<14:03,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 44/474 [01:27<13:54,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512

  9%|▉         | 45/474 [01:29<13:49,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1047 to 1536 to be a multiple of `config.attention_window`: 512

 10%|▉         | 46/474 [01:31<14:22,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 662 to 1024 to be a multiple of `config.attention_window`: 512

 10%|▉         | 47/474 [01:33<14:06,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1062 to 1536 to be a multiple of `config.attention_window`: 512

 10%|█         | 48/474 [01:35<14:33,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512

 10%|█         | 49/474 [01:37<14:13,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1026 to 1536 to be a multiple of `config.attention_window`: 512

 11%|█         | 50/474 [01:39<14:37,  2.07s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█         | 51/474 [01:41<14:07,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█         | 52/474 [01:43<13:53,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 999 to 1024 to be a multiple of `config.attention_window`: 512

 11%|█         | 53/474 [01:45<13:42,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512

 11%|█▏        | 54/474 [01:47<14:11,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 55/474 [01:49<14:31,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 56/474 [01:51<14:07,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 786 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 57/474 [01:53<13:49,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 58/474 [01:55<13:37,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1076 to 1536 to be a multiple of `config.attention_window`: 512

 12%|█▏        | 59/474 [01:57<14:04,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 60/474 [01:59<13:09,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 947 to 1024 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 61/474 [02:00<13:06,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 62/474 [02:02<13:04,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512

 13%|█▎        | 63/474 [02:04<13:02,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▎        | 64/474 [02:06<13:00,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▎        | 65/474 [02:08<12:59,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▍        | 66/474 [02:10<11:56,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512

 14%|█▍        | 67/474 [02:11<12:12,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512

 14%|█▍        | 68/474 [02:13<11:46,  1.74s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512

 15%|█▍        | 69/474 [02:15<12:05,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 991 to 1024 to be a multiple of `config.attention_window`: 512

 15%|█▍        | 70/474 [02:17<12:18,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512

 15%|█▍        | 71/474 [02:19<13:02,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 765 to 1024 to be a multiple of `config.attention_window`: 512

 15%|█▌        | 72/474 [02:21<12:57,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1096 to 1536 to be a multiple of `config.attention_window`: 512

 15%|█▌        | 73/474 [02:23<13:28,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 936 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 74/474 [02:25<13:13,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 75/474 [02:27<13:01,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 898 to 1024 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 76/474 [02:29<12:30,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1037 to 1536 to be a multiple of `config.attention_window`: 512

 16%|█▌        | 77/474 [02:31<13:05,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1084 to 1536 to be a multiple of `config.attention_window`: 512

 16%|█▋        | 78/474 [02:33<13:29,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 79/474 [02:35<12:45,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1062 to 1536 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 80/474 [02:37<13:14,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1001 to 1024 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 81/474 [02:39<12:59,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 17%|█▋        | 82/474 [02:41<13:23,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 875 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 83/474 [02:43<13:03,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 84/474 [02:45<12:50,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 85/474 [02:47<12:40,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 787 to 1024 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 86/474 [02:49<12:31,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512

 18%|█▊        | 87/474 [02:50<11:50,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▊        | 88/474 [02:52<11:56,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 89/474 [02:54<12:34,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 90/474 [02:56<12:27,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 91/474 [02:58<12:20,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512

 19%|█▉        | 92/474 [03:00<12:15,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 987 to 1024 to be a multiple of `config.attention_window`: 512

 20%|█▉        | 93/474 [03:02<12:12,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512

 20%|█▉        | 94/474 [03:04<12:08,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1047 to 1536 to be a multiple of `config.attention_window`: 512

 20%|██        | 95/474 [03:06<12:40,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 778 to 1024 to be a multiple of `config.attention_window`: 512

 20%|██        | 96/474 [03:08<12:27,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 779 to 1024 to be a multiple of `config.attention_window`: 512

 20%|██        | 97/474 [03:10<12:17,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██        | 98/474 [03:12<12:10,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██        | 99/474 [03:14<12:03,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...

 21%|██        | 100/474 [03:16<11:58,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 512

 21%|██▏       | 101/474 [03:18<11:54,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1006 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 102/474 [03:19<11:52,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 103/474 [03:22<12:22,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 104/474 [03:24<12:11,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1021 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 105/474 [03:26<12:02,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 968 to 1024 to be a multiple of `config.attention_window`: 512

 22%|██▏       | 106/474 [03:27<11:53,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1043 to 1536 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 107/474 [03:30<12:20,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1070 to 1536 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 108/474 [03:32<12:37,  2.07s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1042 to 1536 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 109/474 [03:34<12:48,  2.11s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1059 to 1536 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 110/474 [03:36<12:56,  2.13s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512

 23%|██▎       | 111/474 [03:38<11:26,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▎       | 112/474 [03:39<11:26,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 113/474 [03:41<11:25,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 114/474 [03:44<11:57,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 115/474 [03:45<11:33,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1067 to 1536 to be a multiple of `config.attention_window`: 512

 24%|██▍       | 116/474 [03:48<12:01,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512

 25%|██▍       | 117/474 [03:49<11:47,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1044 to 1536 to be a multiple of `config.attention_window`: 512

 25%|██▍       | 118/474 [03:52<12:10,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1046 to 1536 to be a multiple of `config.attention_window`: 512

 25%|██▌       | 119/474 [03:54<12:23,  2.09s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1008 to 1024 to be a multiple of `config.attention_window`: 512

 25%|██▌       | 120/474 [03:56<12:00,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1054 to 1536 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 121/474 [03:58<12:15,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 122/474 [04:00<12:25,  2.12s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 123/474 [04:02<11:59,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▌       | 124/474 [04:04<11:41,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512

 26%|██▋       | 125/474 [04:06<11:28,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 126/474 [04:07<10:47,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 905 to 1024 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 127/474 [04:09<10:49,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 869 to 1024 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 128/474 [04:11<10:45,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 129/474 [04:13<10:47,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512

 27%|██▋       | 130/474 [04:15<11:18,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 131/474 [04:17<11:09,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1005 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 132/474 [04:19<11:03,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 133/474 [04:21<10:57,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 965 to 1024 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 134/474 [04:23<10:52,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512

 28%|██▊       | 135/474 [04:25<11:19,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512

 29%|██▊       | 136/474 [04:27<11:38,  2.07s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1002 to 1024 to be a multiple of `config.attention_window`: 512

 29%|██▉       | 137/474 [04:29<11:20,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 983 to 1024 to be a multiple of `config.attention_window`: 512

 29%|██▉       | 138/474 [04:31<11:07,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512

 29%|██▉       | 139/474 [04:32<09:38,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 722 to 1024 to be a multiple of `config.attention_window`: 512

 30%|██▉       | 140/474 [04:34<09:54,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 683 to 1024 to be a multiple of `config.attention_window`: 512

 30%|██▉       | 141/474 [04:36<10:06,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1029 to 1536 to be a multiple of `config.attention_window`: 512

 30%|██▉       | 142/474 [04:38<10:43,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 736 to 1024 to be a multiple of `config.attention_window`: 512

 30%|███       | 143/474 [04:40<10:37,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 447 to 512 to be a multiple of `config.attention_window`: 512

 30%|███       | 144/474 [04:42<10:04,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512

 31%|███       | 145/474 [04:43<09:40,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███       | 146/474 [04:45<09:52,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███       | 147/474 [04:47<10:00,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███       | 148/474 [04:49<10:05,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 924 to 1024 to be a multiple of `config.attention_window`: 512

 31%|███▏      | 149/474 [04:51<10:07,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 150/474 [04:53<10:08,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 151/474 [04:55<10:37,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 152/474 [04:57<09:41,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 153/474 [04:58<09:48,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 988 to 1024 to be a multiple of `config.attention_window`: 512

 32%|███▏      | 154/474 [05:00<09:52,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 155/474 [05:02<09:55,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 874 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 156/474 [05:04<09:56,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 157/474 [05:06<09:56,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512

 33%|███▎      | 158/474 [05:08<10:24,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512

 34%|███▎      | 159/474 [05:10<09:46,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1062 to 1536 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 160/474 [05:12<10:15,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 765 to 1024 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 161/474 [05:14<10:08,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 711 to 1024 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 162/474 [05:16<10:02,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 775 to 1024 to be a multiple of `config.attention_window`: 512

 34%|███▍      | 163/474 [05:18<09:58,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▍      | 164/474 [05:20<09:54,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▍      | 165/474 [05:21<09:11,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1044 to 1536 to be a multiple of `config.attention_window`: 512

 35%|███▌      | 166/474 [05:23<09:48,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512

 35%|███▌      | 167/474 [05:25<09:45,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512

 35%|███▌      | 168/474 [05:27<10:10,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512

 36%|███▌      | 169/474 [05:30<10:26,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 36%|███▌      | 170/474 [05:32<10:37,  2.10s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512

 36%|███▌      | 171/474 [05:34<10:44,  2.13s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 891 to 1024 to be a multiple of `config.attention_window`: 512

 36%|███▋      | 172/474 [05:36<10:21,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1055 to 1536 to be a multiple of `config.attention_window`: 512

 36%|███▋      | 173/474 [05:38<10:32,  2.10s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 174/474 [05:40<10:12,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 175/474 [05:42<09:58,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 983 to 1024 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 176/474 [05:44<09:48,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 37%|███▋      | 177/474 [05:46<09:48,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 178/474 [05:48<09:39,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 179/474 [05:50<09:32,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 180/474 [05:52<09:27,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1043 to 1536 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 181/474 [05:54<09:49,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 775 to 1024 to be a multiple of `config.attention_window`: 512

 38%|███▊      | 182/474 [05:56<09:37,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1038 to 1536 to be a multiple of `config.attention_window`: 512

 39%|███▊      | 183/474 [05:58<09:54,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1050 to 1536 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 184/474 [06:00<10:05,  2.09s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 880 to 1024 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 185/474 [06:02<09:47,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 963 to 1024 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 186/474 [06:03<08:51,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512

 39%|███▉      | 187/474 [06:06<09:20,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 756 to 1024 to be a multiple of `config.attention_window`: 512

 40%|███▉      | 188/474 [06:07<09:14,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1006 to 1024 to be a multiple of `config.attention_window`: 512

 40%|███▉      | 189/474 [06:09<09:10,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512

 40%|████      | 190/474 [06:11<09:07,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1015 to 1024 to be a multiple of `config.attention_window`: 512

 40%|████      | 191/474 [06:13<09:03,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1033 to 1536 to be a multiple of `config.attention_window`: 512

 41%|████      | 192/474 [06:15<09:24,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1064 to 1536 to be a multiple of `config.attention_window`: 512

 41%|████      | 193/474 [06:18<09:39,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512

 41%|████      | 194/474 [06:19<09:24,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1068 to 1536 to be a multiple of `config.attention_window`: 512

 41%|████      | 195/474 [06:22<09:37,  2.07s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1065 to 1536 to be a multiple of `config.attention_window`: 512

 41%|████▏     | 196/474 [06:24<09:46,  2.11s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 913 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 197/474 [06:26<09:26,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 198/474 [06:28<09:12,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 199/474 [06:30<09:02,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1000 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 200/474 [06:31<08:56,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512

 42%|████▏     | 201/474 [06:33<08:50,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1039 to 1536 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 202/474 [06:36<09:10,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 203/474 [06:38<08:59,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1051 to 1536 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 204/474 [06:40<09:15,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1007 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 205/474 [06:42<09:01,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512

 43%|████▎     | 206/474 [06:43<08:42,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 817 to 1024 to be a multiple of `config.attention_window`: 512

 44%|████▎     | 207/474 [06:45<08:36,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 883 to 1024 to be a multiple of `config.attention_window`: 512

 44%|████▍     | 208/474 [06:47<08:31,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1063 to 1536 to be a multiple of `config.attention_window`: 512

 44%|████▍     | 209/474 [06:49<08:50,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 904 to 1024 to be a multiple of `config.attention_window`: 512

 44%|████▍     | 210/474 [06:51<08:40,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1053 to 1536 to be a multiple of `config.attention_window`: 512

 45%|████▍     | 211/474 [06:54<08:55,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▍     | 212/474 [06:55<08:42,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▍     | 213/474 [06:57<08:32,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 836 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▌     | 214/474 [06:59<08:25,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 758 to 1024 to be a multiple of `config.attention_window`: 512

 45%|████▌     | 215/474 [07:01<08:19,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 216/474 [07:03<08:33,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 774 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 217/474 [07:05<08:19,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 723 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 218/474 [07:07<08:14,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512

 46%|████▌     | 219/474 [07:09<08:33,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1001 to 1024 to be a multiple of `config.attention_window`: 512

 46%|████▋     | 220/474 [07:11<08:22,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 986 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 221/474 [07:13<08:15,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 222/474 [07:15<08:09,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1035 to 1536 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 223/474 [07:17<08:26,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1064 to 1536 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 224/474 [07:19<08:38,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512

 47%|████▋     | 225/474 [07:21<08:46,  2.11s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 226/474 [07:23<08:28,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 227/474 [07:25<08:15,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 228/474 [07:27<08:05,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1060 to 1536 to be a multiple of `config.attention_window`: 512

 48%|████▊     | 229/474 [07:29<08:20,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 726 to 1024 to be a multiple of `config.attention_window`: 512

 49%|████▊     | 230/474 [07:31<08:07,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512

 49%|████▊     | 231/474 [07:33<08:20,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 745 to 1024 to be a multiple of `config.attention_window`: 512

 49%|████▉     | 232/474 [07:35<08:06,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512

 49%|████▉     | 233/474 [07:37<07:56,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1063 to 1536 to be a multiple of `config.attention_window`: 512

 49%|████▉     | 234/474 [07:39<08:10,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512

 50%|████▉     | 235/474 [07:41<07:58,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512

 50%|████▉     | 236/474 [07:44<08:10,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1021 to 1024 to be a multiple of `config.attention_window`: 512

 50%|█████     | 237/474 [07:45<07:54,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 50%|█████     | 238/474 [07:48<08:07,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 832 to 1024 to be a multiple of `config.attention_window`: 512

 50%|█████     | 239/474 [07:50<07:54,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████     | 240/474 [07:51<07:43,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████     | 241/474 [07:53<07:35,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512

 51%|█████     | 242/474 [07:56<07:50,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████▏    | 243/474 [07:57<07:39,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512

 51%|█████▏    | 244/474 [07:59<07:31,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...

 52%|█████▏    | 245/474 [08:01<07:26,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 246/474 [08:03<07:41,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 247/474 [08:06<07:51,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 970 to 1024 to be a multiple of `config.attention_window`: 512

 52%|█████▏    | 248/474 [08:07<07:25,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 249/474 [08:09<07:18,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 250/474 [08:11<07:14,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1059 to 1536 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 251/474 [08:13<07:29,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 252/474 [08:15<07:19,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512

 53%|█████▎    | 253/474 [08:17<07:12,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▎    | 254/474 [08:19<07:06,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 255/474 [08:21<06:55,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 256/474 [08:23<06:53,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1052 to 1536 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 257/474 [08:25<07:12,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 975 to 1024 to be a multiple of `config.attention_window`: 512

 54%|█████▍    | 258/474 [08:27<07:05,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 55%|█████▍    | 259/474 [08:29<07:18,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512

 55%|█████▍    | 260/474 [08:31<07:27,  2.09s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512

 55%|█████▌    | 261/474 [08:33<07:14,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512

 55%|█████▌    | 262/474 [08:35<07:23,  2.09s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512

 55%|█████▌    | 263/474 [08:37<07:09,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512

 56%|█████▌    | 264/474 [08:39<06:40,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1051 to 1536 to be a multiple of `config.attention_window`: 512

 56%|█████▌    | 265/474 [08:41<06:57,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1019 to 1024 to be a multiple of `config.attention_window`: 512

 56%|█████▌    | 266/474 [08:43<06:49,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1068 to 1536 to be a multiple of `config.attention_window`: 512

 56%|█████▋    | 267/474 [08:45<07:01,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1026 to 1536 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 268/474 [08:48<07:08,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 269/474 [08:49<06:55,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1054 to 1536 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 270/474 [08:52<07:03,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1013 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 271/474 [08:53<06:50,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 712 to 1024 to be a multiple of `config.attention_window`: 512

 57%|█████▋    | 272/474 [08:55<06:40,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 273/474 [08:58<06:51,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 274/474 [08:59<06:40,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 275/474 [09:01<06:32,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1054 to 1536 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 276/474 [09:04<06:43,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512

 58%|█████▊    | 277/474 [09:06<06:50,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 740 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▊    | 278/474 [09:08<06:37,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1037 to 1536 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 279/474 [09:10<06:45,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 742 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 280/474 [09:12<06:32,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1100 to 1536 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 281/474 [09:14<06:40,  2.07s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512

 59%|█████▉    | 282/474 [09:16<06:27,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512

 60%|█████▉    | 283/474 [09:17<06:01,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1085 to 1536 to be a multiple of `config.attention_window`: 512

 60%|█████▉    | 284/474 [09:20<06:17,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 901 to 1024 to be a multiple of `config.attention_window`: 512

 60%|██████    | 285/474 [09:22<06:10,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1045 to 1536 to be a multiple of `config.attention_window`: 512

 60%|██████    | 286/474 [09:24<06:21,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1060 to 1536 to be a multiple of `config.attention_window`: 512

 61%|██████    | 287/474 [09:26<06:29,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512

 61%|██████    | 288/474 [09:28<06:00,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1053 to 1536 to be a multiple of `config.attention_window`: 512

 61%|██████    | 289/474 [09:30<06:12,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512

 61%|██████    | 290/474 [09:32<06:04,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512

 61%|██████▏   | 291/474 [09:34<06:14,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 993 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 292/474 [09:36<06:04,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 293/474 [09:38<05:56,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 294/474 [09:39<05:35,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 758 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 295/474 [09:41<05:35,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 930 to 1024 to be a multiple of `config.attention_window`: 512

 62%|██████▏   | 296/474 [09:43<05:34,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 297/474 [09:45<05:18,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 298/474 [09:47<05:21,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 299/474 [09:48<05:07,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 766 to 1024 to be a multiple of `config.attention_window`: 512

 63%|██████▎   | 300/474 [09:50<05:13,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512

 64%|██████▎   | 301/474 [09:51<04:30,  1.56s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1029 to 1536 to be a multiple of `config.attention_window`: 512

 64%|██████▎   | 302/474 [09:53<05:01,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 984 to 1024 to be a multiple of `config.attention_window`: 512

 64%|██████▍   | 303/474 [09:55<05:07,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1048 to 1536 to be a multiple of `config.attention_window`: 512

 64%|██████▍   | 304/474 [09:57<05:26,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...

 64%|██████▍   | 305/474 [09:59<05:07,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▍   | 306/474 [10:01<05:05,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▍   | 307/474 [10:03<05:08,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▍   | 308/474 [10:05<05:09,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 985 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▌   | 309/474 [10:06<05:10,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 967 to 1024 to be a multiple of `config.attention_window`: 512

 65%|██████▌   | 310/474 [10:08<05:09,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 728 to 1024 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 311/474 [10:10<05:09,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 312/474 [10:12<04:56,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 313/474 [10:14<04:44,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1067 to 1536 to be a multiple of `config.attention_window`: 512

 66%|██████▌   | 314/474 [10:16<05:03,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512

 66%|██████▋   | 315/474 [10:17<04:47,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1007 to 1024 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 316/474 [10:19<04:49,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 984 to 1024 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 317/474 [10:21<04:50,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 318/474 [10:22<04:13,  1.63s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512

 67%|██████▋   | 319/474 [10:24<04:25,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 978 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 320/474 [10:26<04:27,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 321/474 [10:28<04:33,  1.79s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 322/474 [10:29<04:22,  1.73s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 323/474 [10:31<04:28,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512

 68%|██████▊   | 324/474 [10:33<04:32,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512

 69%|██████▊   | 325/474 [10:35<04:34,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1086 to 1536 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 326/474 [10:37<04:48,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 327/474 [10:39<04:31,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 932 to 1024 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 328/474 [10:41<04:31,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512

 69%|██████▉   | 329/474 [10:42<04:16,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1065 to 1536 to be a multiple of `config.attention_window`: 512

 70%|██████▉   | 330/474 [10:45<04:33,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 512

 70%|██████▉   | 331/474 [10:47<04:31,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512

 70%|███████   | 332/474 [10:48<04:29,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 781 to 1024 to be a multiple of `config.attention_window`: 512

 70%|███████   | 333/474 [10:50<04:28,  1.90s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512

 70%|███████   | 334/474 [10:52<04:26,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1009 to 1024 to be a multiple of `config.attention_window`: 512

 71%|███████   | 335/474 [10:54<04:25,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1082 to 1536 to be a multiple of `config.attention_window`: 512

 71%|███████   | 336/474 [10:56<04:35,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1000 to 1024 to be a multiple of `config.attention_window`: 512

 71%|███████   | 337/474 [10:58<04:29,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1012 to 1024 to be a multiple of `config.attention_window`: 512

 71%|███████▏  | 338/474 [11:00<04:25,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 723 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 339/474 [11:02<04:21,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 775 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 340/474 [11:04<04:17,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 341/474 [11:06<04:01,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 726 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 342/474 [11:07<04:02,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 760 to 1024 to be a multiple of `config.attention_window`: 512

 72%|███████▏  | 343/474 [11:09<04:03,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1059 to 1536 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 344/474 [11:12<04:14,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 345/474 [11:14<04:22,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 962 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 346/474 [11:16<04:15,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 711 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 347/474 [11:18<04:09,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1013 to 1024 to be a multiple of `config.attention_window`: 512

 73%|███████▎  | 348/474 [11:19<04:05,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▎  | 349/474 [11:21<03:41,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 774 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 350/474 [11:23<03:42,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 908 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 351/474 [11:25<03:44,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 352/474 [11:26<03:34,  1.76s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1008 to 1024 to be a multiple of `config.attention_window`: 512

 74%|███████▍  | 353/474 [11:28<03:38,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1039 to 1536 to be a multiple of `config.attention_window`: 512

 75%|███████▍  | 354/474 [11:30<03:50,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512

 75%|███████▍  | 355/474 [11:32<03:58,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 696 to 1024 to be a multiple of `config.attention_window`: 512

 75%|███████▌  | 356/474 [11:34<03:53,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512

 75%|███████▌  | 357/474 [11:36<03:49,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1022 to 1024 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 358/474 [11:38<03:45,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 718 to 1024 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 359/474 [11:40<03:42,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 360/474 [11:42<03:29,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512

 76%|███████▌  | 361/474 [11:43<03:19,  1.77s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512

 76%|███████▋  | 362/474 [11:45<03:22,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 706 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 363/474 [11:47<03:24,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 364/474 [11:49<03:07,  1.71s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 962 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 365/474 [11:50<03:02,  1.67s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1059 to 1536 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 366/474 [11:52<03:17,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512

 77%|███████▋  | 367/474 [11:54<03:14,  1.82s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 368/474 [11:56<03:15,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 707 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 369/474 [11:58<03:15,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 370/474 [12:00<03:14,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 657 to 1024 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 371/474 [12:02<03:13,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512

 78%|███████▊  | 372/474 [12:04<03:21,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512

 79%|███████▊  | 373/474 [12:06<03:17,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512

 79%|███████▉  | 374/474 [12:07<03:04,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512

 79%|███████▉  | 375/474 [12:09<03:04,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1034 to 1536 to be a multiple of `config.attention_window`: 512

 79%|███████▉  | 376/474 [12:12<03:12,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 968 to 1024 to be a multiple of `config.attention_window`: 512

 80%|███████▉  | 377/474 [12:13<03:08,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 512

 80%|███████▉  | 378/474 [12:15<03:05,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512

 80%|███████▉  | 379/474 [12:17<02:54,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 944 to 1024 to be a multiple of `config.attention_window`: 512

 80%|████████  | 380/474 [12:19<02:54,  1.85s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512

 80%|████████  | 381/474 [12:20<02:45,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512

 81%|████████  | 382/474 [12:22<02:46,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512

 81%|████████  | 383/474 [12:24<02:39,  1.75s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1092 to 1536 to be a multiple of `config.attention_window`: 512

 81%|████████  | 384/474 [12:26<02:49,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512

 81%|████████  | 385/474 [12:28<02:48,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1171 to 1536 to be a multiple of `config.attention_window`: 512

 81%|████████▏ | 386/474 [12:30<02:54,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1035 to 1536 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 387/474 [12:32<02:58,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 388/474 [12:34<02:52,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 791 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 389/474 [12:36<02:48,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1071 to 1536 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 390/474 [12:38<02:52,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512

 82%|████████▏ | 391/474 [12:40<02:46,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1020 to 1024 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 392/474 [12:42<02:42,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 393/474 [12:44<02:31,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 394/474 [12:46<02:30,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512

 83%|████████▎ | 395/474 [12:47<02:20,  1.78s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 781 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▎ | 396/474 [12:49<02:21,  1.81s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 397/474 [12:51<02:21,  1.84s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 398/474 [12:53<02:21,  1.86s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1023 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 399/474 [12:55<02:20,  1.87s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512

 84%|████████▍ | 400/474 [12:57<02:19,  1.88s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512

 85%|████████▍ | 401/474 [12:58<02:11,  1.80s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 982 to 1024 to be a multiple of `config.attention_window`: 512

 85%|████████▍ | 402/474 [13:00<02:11,  1.83s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1026 to 1536 to be a multiple of `config.attention_window`: 512

 85%|████████▌ | 403/474 [13:03<02:17,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 734 to 1024 to be a multiple of `config.attention_window`: 512

 85%|████████▌ | 404/474 [13:04<02:15,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1053 to 1536 to be a multiple of `config.attention_window`: 512

 85%|████████▌ | 405/474 [13:07<02:19,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1057 to 1536 to be a multiple of `config.attention_window`: 512

 86%|████████▌ | 406/474 [13:09<02:21,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 882 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▌ | 407/474 [13:11<02:15,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1068 to 1536 to be a multiple of `config.attention_window`: 512

 86%|████████▌ | 408/474 [13:13<02:17,  2.08s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 986 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▋ | 409/474 [13:15<02:11,  2.03s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 673 to 1024 to be a multiple of `config.attention_window`: 512

 86%|████████▋ | 410/474 [13:17<02:01,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1021 to 1024 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 411/474 [13:18<02:00,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1039 to 1536 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 412/474 [13:21<02:03,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 750 to 1024 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 413/474 [13:23<02:00,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512

 87%|████████▋ | 414/474 [13:24<01:57,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 943 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 415/474 [13:26<01:54,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 416/474 [13:28<01:51,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1052 to 1536 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 417/474 [13:30<01:54,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1059 to 1536 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 418/474 [13:33<01:55,  2.07s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1017 to 1024 to be a multiple of `config.attention_window`: 512

 88%|████████▊ | 419/474 [13:35<01:51,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1005 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▊ | 420/474 [13:37<01:47,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1053 to 1536 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 421/474 [13:39<01:48,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 782 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 422/474 [13:41<01:44,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 965 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 423/474 [13:43<01:40,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 512

 89%|████████▉ | 424/474 [13:44<01:37,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512

 90%|████████▉ | 425/474 [13:46<01:34,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1002 to 1024 to be a multiple of `config.attention_window`: 512

 90%|████████▉ | 426/474 [13:48<01:32,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 758 to 1024 to be a multiple of `config.attention_window`: 512

 90%|█████████ | 427/474 [13:50<01:30,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1013 to 1024 to be a multiple of `config.attention_window`: 512

 90%|█████████ | 428/474 [13:52<01:28,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 429/474 [13:54<01:28,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 999 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 430/474 [13:56<01:25,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 431/474 [13:58<01:23,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512

 91%|█████████ | 432/474 [14:00<01:20,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1057 to 1536 to be a multiple of `config.attention_window`: 512

 91%|█████████▏| 433/474 [14:02<01:22,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 434/474 [14:04<01:18,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1022 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 435/474 [14:06<01:16,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 436/474 [14:08<01:13,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 992 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 437/474 [14:10<01:11,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512

 92%|█████████▏| 438/474 [14:12<01:08,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1000 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 439/474 [14:13<01:06,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 989 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 440/474 [14:15<01:04,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1067 to 1536 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 441/474 [14:18<01:05,  2.00s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 996 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 442/474 [14:19<01:02,  1.97s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512

 93%|█████████▎| 443/474 [14:21<01:00,  1.95s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512

 94%|█████████▎| 444/474 [14:23<01:00,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512

 94%|█████████▍| 445/474 [14:25<00:57,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 984 to 1024 to be a multiple of `config.attention_window`: 512

 94%|█████████▍| 446/474 [14:27<00:54,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 974 to 1024 to be a multiple of `config.attention_window`: 512

 94%|█████████▍| 447/474 [14:29<00:52,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▍| 448/474 [14:31<00:50,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512

 95%|█████████▍| 449/474 [14:33<00:50,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1069 to 1536 to be a multiple of `config.attention_window`: 512

 95%|█████████▍| 450/474 [14:35<00:49,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▌| 451/474 [14:37<00:46,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 512

 95%|█████████▌| 452/474 [14:39<00:43,  1.98s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1070 to 1536 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 453/474 [14:41<00:42,  2.05s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1073 to 1536 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 454/474 [14:44<00:41,  2.09s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1082 to 1536 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 455/474 [14:46<00:40,  2.12s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512

 96%|█████████▌| 456/474 [14:48<00:37,  2.06s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512

 96%|█████████▋| 457/474 [14:50<00:34,  2.01s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 458/474 [14:51<00:30,  1.89s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1030 to 1536 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 459/474 [14:54<00:29,  1.99s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 460/474 [14:55<00:27,  1.96s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1151 to 1536 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 461/474 [14:58<00:26,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1026 to 1536 to be a multiple of `config.attention_window`: 512

 97%|█████████▋| 462/474 [15:00<00:25,  2.09s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 463/474 [15:02<00:22,  2.04s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1072 to 1536 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 464/474 [15:04<00:20,  2.09s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 465/474 [15:06<00:17,  1.94s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512

 98%|█████████▊| 466/474 [15:07<00:15,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1052 to 1536 to be a multiple of `config.attention_window`: 512

 99%|█████████▊| 467/474 [15:10<00:14,  2.02s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1041 to 1536 to be a multiple of `config.attention_window`: 512

 99%|█████████▊| 468/474 [15:12<00:12,  2.07s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512

 99%|█████████▉| 469/474 [15:13<00:09,  1.93s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 623 to 1024 to be a multiple of `config.attention_window`: 512

 99%|█████████▉| 470/474 [15:15<00:07,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512

 99%|█████████▉| 471/474 [15:17<00:05,  1.92s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 695 to 1024 to be a multiple of `config.attention_window`: 512

100%|█████████▉| 472/474 [15:19<00:03,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512

100%|█████████▉| 473/474 [15:21<00:01,  1.91s/it][AINFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512

100%|██████████| 474/474 [15:23<00:00,  1.81s/it][A100%|██████████| 474/474 [15:23<00:00,  1.95s/it]
INFO:__main__:Evaluated [step 100] rewards = -2.2798
INFO:__main__:Best ckpt updated to [step 100]
INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1080 to 1536 to be a multiple of `config.attention_window`: 512
 80%|████████  | 101/126 [1:08:04<2:03:06, 295.48s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 857 to 1024 to be a multiple of `config.attention_window`: 512
 81%|████████  | 102/126 [1:08:20<1:24:42, 211.79s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512
 82%|████████▏ | 103/126 [1:08:37<58:43, 153.20s/it]  INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512
 83%|████████▎ | 104/126 [1:08:54<41:14, 112.49s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512
 83%|████████▎ | 105/126 [1:09:11<29:17, 83.69s/it] INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512
 84%|████████▍ | 106/126 [1:09:27<21:10, 63.54s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1054 to 1536 to be a multiple of `config.attention_window`: 512
 85%|████████▍ | 107/126 [1:09:45<15:44, 49.73s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512
 86%|████████▌ | 108/126 [1:10:01<11:55, 39.77s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512
 87%|████████▋ | 109/126 [1:10:19<09:22, 33.11s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512
INFO:__main__:KL divergence 10.342033386230469 exceeds threshold 10.0
 87%|████████▋ | 110/126 [1:10:35<07:30, 28.13s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512
 88%|████████▊ | 111/126 [1:10:51<06:07, 24.50s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1025 to 1536 to be a multiple of `config.attention_window`: 512
 89%|████████▉ | 112/126 [1:11:09<05:13, 22.41s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1055 to 1536 to be a multiple of `config.attention_window`: 512
 90%|████████▉ | 113/126 [1:11:26<04:32, 20.94s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512
 90%|█████████ | 114/126 [1:11:43<03:55, 19.62s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1076 to 1536 to be a multiple of `config.attention_window`: 512
 91%|█████████▏| 115/126 [1:12:00<03:28, 18.99s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512
 92%|█████████▏| 116/126 [1:12:17<03:02, 18.25s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512
 93%|█████████▎| 117/126 [1:12:33<02:39, 17.72s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1039 to 1536 to be a multiple of `config.attention_window`: 512
 94%|█████████▎| 118/126 [1:12:51<02:21, 17.66s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 947 to 1024 to be a multiple of `config.attention_window`: 512
 94%|█████████▍| 119/126 [1:13:07<02:01, 17.32s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 752 to 1024 to be a multiple of `config.attention_window`: 512
 95%|█████████▌| 120/126 [1:13:24<01:42, 17.08s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 752 to 1024 to be a multiple of `config.attention_window`: 512
 96%|█████████▌| 121/126 [1:13:40<01:24, 16.92s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 705 to 1024 to be a multiple of `config.attention_window`: 512
 97%|█████████▋| 122/126 [1:13:57<01:07, 16.80s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 512
 98%|█████████▊| 123/126 [1:14:13<00:50, 16.72s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512
 98%|█████████▊| 124/126 [1:14:30<00:33, 16.66s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 1064 to 1536 to be a multiple of `config.attention_window`: 512
 99%|█████████▉| 125/126 [1:14:48<00:16, 16.92s/it]INFO:src.utils.my_longformer:Initializing global attention on CLS token...
INFO:src.utils.my_longformer:Input ids are automatically padded from 986 to 1024 to be a multiple of `config.attention_window`: 512
100%|██████████| 126/126 [1:15:04<00:00, 16.80s/it]100%|██████████| 126/126 [1:15:04<00:00, 35.75s/it]
wandb: - 18.467 MB of 18.467 MB uploadedwandb: \ 18.673 MB of 18.673 MB uploadedwandb: 
wandb: Run history:
wandb: eval/completeness_rewards ▁▅█
wandb:              eval/lengths ▁▅█
wandb:              eval/rewards ▁▅█
wandb:                eval/rouge █▅▁
wandb:                 eval/step ▁▅█
wandb:         train/loss/policy ▆▄▄▆▄▅▄▃▄▃▄█▄▃▃▃▄▃▂▂▃▃▂▁▄▃▃▃▄▂▄▃▄▃▃▃▂▃▂▃
wandb:          train/loss/total █▇▇█▆▇▆▆▅▆▆▆▅▄▄▄▅▄▅▄▄▄▃▃▄▄▃▃▃▂▂▂▂▁▂▂▁▂▁▂
wandb:          train/loss/value █▇▇█▆▇▆▆▅▆▆▅▅▄▄▅▅▄▅▄▄▄▃▄▄▄▃▃▃▂▂▂▂▁▂▂▁▂▁▁
wandb:                  train/lr ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███▇▆▅▄▄▃▂▁
wandb:           train/reward/KL ▁▁▁▁▁▁▁▁▂▂▂▁▂▂▃▄▄▅▅▃▅▅▄▃▄▆▆▇█▅▇▇▆▇▅▇▄▇▃▅
wandb:    train/reward/penalized ██████████▇█▇▇▆▅▅▄▄▆▄▄▅▆▅▃▄▂▁▄▂▂▃▂▄▂▅▂▆▄
wandb:          train/reward/raw ▁▄▄▄▄▄▄▃▇▅▃▄▄██▂▃▅▄▅▆▁▆▁▄▄▇▄▄▅▆▄▅▅▃▅▆▄▄▄
wandb:                train/step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:              eval/step 100
wandb:      train/loss/policy 0.01656
wandb:       train/loss/total 0.36805
wandb:       train/loss/value 0.35148
wandb:               train/lr 0.0
wandb:        train/reward/KL 5.04472
wandb: train/reward/penalized -3.80911
wandb:       train/reward/raw -2.2957
wandb:             train/step 125
wandb: 
wandb: 🚀 View run rlhf at: https://wandb.ai/ouroboroos/hw7/runs/qi7kage1
wandb: Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240402_191931-qi7kage1/logs
